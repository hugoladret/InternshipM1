{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018-06-08 - Comparaison modèle/humain\n",
    "# Pour génerer les MC  : 2018-06-17 Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le Ring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze layers after and including freezing_layer+1 (layers start at 0)\n",
    "def freeze_layers(freezing_layer, nn_model = model) :\n",
    "    for count,child in enumerate(model.children()) :\n",
    "        if  count < freezing_layer+1 : #to freeze at iteration 1\n",
    "            print(\"Layer no. %s -- %s -- NOT FROZEN\"% (count,child))\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        else :\n",
    "            print(\"Layer no. %s -- %s -- FROZEN\"%(count, child))\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "#freeze all the layers except the unfrozen one\n",
    "def freeze_all_layers(unfrozen_layer, nn_model = model) :\n",
    "    for count,child in enumerate(model.children()) :\n",
    "        if count == unfrozen_layer :\n",
    "            print(\"Layer no. %s -- %s -- NOT FROZEN\"% (count,child))\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        else :\n",
    "            print(\"Layer no. %s -- %s -- FROZEN\"%(count, child))\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "#unfreeze every layers\n",
    "def layers_microwave(nn_model = model) :\n",
    "    for count,child in enumerate(model.children()) :\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiRNN(\n",
      "  (rnn): LSTM(49, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(30, 30), stride=(1, 1))\n",
      "  (conv2): Conv2d(1, 8, kernel_size=(30, 30), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=113, out_features=16, bias=True)\n",
      ")\n",
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:137: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/hugo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:138: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/60], Step [100/480], Loss: 2.0535\n",
      "Epoch [1/60], Step [200/480], Loss: 1.8979\n",
      "Epoch [1/60], Step [300/480], Loss: 1.5753\n",
      "Epoch [1/60], Step [400/480], Loss: 1.5173\n",
      "Epoch [2/60], Step [100/480], Loss: 1.2730\n",
      "Epoch [2/60], Step [200/480], Loss: 1.0720\n",
      "Epoch [2/60], Step [300/480], Loss: 0.8932\n",
      "Epoch [2/60], Step [400/480], Loss: 0.9677\n",
      "Epoch [3/60], Step [100/480], Loss: 1.2443\n",
      "Epoch [3/60], Step [200/480], Loss: 1.1975\n",
      "Epoch [3/60], Step [300/480], Loss: 1.1995\n",
      "Epoch [3/60], Step [400/480], Loss: 0.9247\n",
      "Epoch [4/60], Step [100/480], Loss: 1.0551\n",
      "Epoch [4/60], Step [200/480], Loss: 1.0950\n",
      "Epoch [4/60], Step [300/480], Loss: 0.7580\n",
      "Epoch [4/60], Step [400/480], Loss: 1.0920\n",
      "Epoch [5/60], Step [100/480], Loss: 0.7240\n",
      "Epoch [5/60], Step [200/480], Loss: 1.3468\n",
      "Epoch [5/60], Step [300/480], Loss: 0.9628\n",
      "Epoch [5/60], Step [400/480], Loss: 1.0299\n",
      "Epoch [6/60], Step [100/480], Loss: 1.2001\n",
      "Epoch [6/60], Step [200/480], Loss: 0.8729\n",
      "Epoch [6/60], Step [300/480], Loss: 0.9995\n",
      "Epoch [6/60], Step [400/480], Loss: 1.2112\n",
      "Epoch [7/60], Step [100/480], Loss: 1.4742\n",
      "Epoch [7/60], Step [200/480], Loss: 0.5119\n",
      "Epoch [7/60], Step [300/480], Loss: 1.0095\n",
      "Epoch [7/60], Step [400/480], Loss: 0.6618\n",
      "Epoch [8/60], Step [100/480], Loss: 0.9888\n",
      "Epoch [8/60], Step [200/480], Loss: 0.4994\n",
      "Epoch [8/60], Step [300/480], Loss: 0.4962\n",
      "Epoch [8/60], Step [400/480], Loss: 0.5412\n",
      "Epoch [9/60], Step [100/480], Loss: 1.0303\n",
      "Epoch [9/60], Step [200/480], Loss: 0.8035\n",
      "Epoch [9/60], Step [300/480], Loss: 1.2456\n",
      "Epoch [9/60], Step [400/480], Loss: 0.7615\n",
      "Epoch [10/60], Step [100/480], Loss: 0.6736\n",
      "Epoch [10/60], Step [200/480], Loss: 0.6678\n",
      "Epoch [10/60], Step [300/480], Loss: 0.4325\n",
      "Epoch [10/60], Step [400/480], Loss: 0.2611\n",
      "Epoch [11/60], Step [100/480], Loss: 0.7320\n",
      "Epoch [11/60], Step [200/480], Loss: 0.6432\n",
      "Epoch [11/60], Step [300/480], Loss: 0.4987\n",
      "Epoch [11/60], Step [400/480], Loss: 0.6482\n",
      "Epoch [12/60], Step [100/480], Loss: 1.1793\n",
      "Epoch [12/60], Step [200/480], Loss: 0.4331\n",
      "Epoch [12/60], Step [300/480], Loss: 0.6560\n",
      "Epoch [12/60], Step [400/480], Loss: 0.7886\n",
      "Epoch [13/60], Step [100/480], Loss: 0.6727\n",
      "Epoch [13/60], Step [200/480], Loss: 0.7805\n",
      "Epoch [13/60], Step [300/480], Loss: 0.4279\n",
      "Epoch [13/60], Step [400/480], Loss: 0.7742\n",
      "Epoch [14/60], Step [100/480], Loss: 0.2547\n",
      "Epoch [14/60], Step [200/480], Loss: 0.3114\n",
      "Epoch [14/60], Step [300/480], Loss: 2.1400\n",
      "Epoch [14/60], Step [400/480], Loss: 0.2562\n",
      "Epoch [15/60], Step [100/480], Loss: 0.6003\n",
      "Epoch [15/60], Step [200/480], Loss: 0.2507\n",
      "Epoch [15/60], Step [300/480], Loss: 0.6716\n",
      "Epoch [15/60], Step [400/480], Loss: 0.5157\n",
      "Epoch [16/60], Step [100/480], Loss: 0.4560\n",
      "Epoch [16/60], Step [200/480], Loss: 0.6811\n",
      "Epoch [16/60], Step [300/480], Loss: 0.4740\n",
      "Epoch [16/60], Step [400/480], Loss: 0.2873\n",
      "Epoch [17/60], Step [100/480], Loss: 0.3660\n",
      "Epoch [17/60], Step [200/480], Loss: 0.4003\n",
      "Epoch [17/60], Step [300/480], Loss: 0.4197\n",
      "Epoch [17/60], Step [400/480], Loss: 1.5970\n",
      "Epoch [18/60], Step [100/480], Loss: 0.6923\n",
      "Epoch [18/60], Step [200/480], Loss: 0.2827\n",
      "Epoch [18/60], Step [300/480], Loss: 0.2890\n",
      "Epoch [18/60], Step [400/480], Loss: 0.4958\n",
      "Epoch [19/60], Step [100/480], Loss: 0.5515\n",
      "Epoch [19/60], Step [200/480], Loss: 0.5376\n",
      "Epoch [19/60], Step [300/480], Loss: 0.5387\n",
      "Epoch [19/60], Step [400/480], Loss: 0.2801\n",
      "Epoch [20/60], Step [100/480], Loss: 0.3553\n",
      "Epoch [20/60], Step [200/480], Loss: 0.1582\n",
      "Epoch [20/60], Step [300/480], Loss: 0.1473\n",
      "Epoch [20/60], Step [400/480], Loss: 0.2398\n",
      "Epoch [21/60], Step [100/480], Loss: 0.1885\n",
      "Epoch [21/60], Step [200/480], Loss: 0.2733\n",
      "Epoch [21/60], Step [300/480], Loss: 0.4208\n",
      "Epoch [21/60], Step [400/480], Loss: 0.6152\n",
      "Epoch [22/60], Step [100/480], Loss: 0.9248\n",
      "Epoch [22/60], Step [200/480], Loss: 0.2413\n",
      "Epoch [22/60], Step [300/480], Loss: 0.2665\n",
      "Epoch [22/60], Step [400/480], Loss: 0.3623\n",
      "Epoch [23/60], Step [100/480], Loss: 0.2661\n",
      "Epoch [23/60], Step [200/480], Loss: 0.6116\n",
      "Epoch [23/60], Step [300/480], Loss: 0.3771\n",
      "Epoch [23/60], Step [400/480], Loss: 0.2057\n",
      "Epoch [24/60], Step [100/480], Loss: 0.4287\n",
      "Epoch [24/60], Step [200/480], Loss: 0.2249\n",
      "Epoch [24/60], Step [300/480], Loss: 0.1450\n",
      "Epoch [24/60], Step [400/480], Loss: 0.3243\n",
      "Epoch [25/60], Step [100/480], Loss: 0.0871\n",
      "Epoch [25/60], Step [200/480], Loss: 0.7082\n",
      "Epoch [25/60], Step [300/480], Loss: 0.8678\n",
      "Epoch [25/60], Step [400/480], Loss: 0.1668\n",
      "Epoch [26/60], Step [100/480], Loss: 0.2343\n",
      "Epoch [26/60], Step [200/480], Loss: 0.1280\n",
      "Epoch [26/60], Step [300/480], Loss: 1.8951\n",
      "Epoch [26/60], Step [400/480], Loss: 0.1356\n",
      "Epoch [27/60], Step [100/480], Loss: 0.4734\n",
      "Epoch [27/60], Step [200/480], Loss: 0.1381\n",
      "Epoch [27/60], Step [300/480], Loss: 0.7804\n",
      "Epoch [27/60], Step [400/480], Loss: 0.5520\n",
      "Epoch [28/60], Step [100/480], Loss: 0.0594\n",
      "Epoch [28/60], Step [200/480], Loss: 0.0888\n",
      "Epoch [28/60], Step [300/480], Loss: 0.6714\n",
      "Epoch [28/60], Step [400/480], Loss: 0.4520\n",
      "Epoch [29/60], Step [100/480], Loss: 0.3557\n",
      "Epoch [29/60], Step [200/480], Loss: 0.2047\n",
      "Epoch [29/60], Step [300/480], Loss: 0.7010\n",
      "Epoch [29/60], Step [400/480], Loss: 0.5558\n",
      "Epoch [30/60], Step [100/480], Loss: 0.1853\n",
      "Epoch [30/60], Step [200/480], Loss: 0.2292\n",
      "Epoch [30/60], Step [300/480], Loss: 0.2128\n",
      "Epoch [30/60], Step [400/480], Loss: 0.3744\n",
      "Epoch [31/60], Step [100/480], Loss: 0.4484\n",
      "Epoch [31/60], Step [200/480], Loss: 0.2089\n",
      "Epoch [31/60], Step [300/480], Loss: 0.2650\n",
      "Epoch [31/60], Step [400/480], Loss: 0.2666\n",
      "Epoch [32/60], Step [100/480], Loss: 0.3160\n",
      "Epoch [32/60], Step [200/480], Loss: 0.4540\n",
      "Epoch [32/60], Step [300/480], Loss: 0.2168\n",
      "Epoch [32/60], Step [400/480], Loss: 0.1986\n",
      "Epoch [33/60], Step [100/480], Loss: 0.2619\n",
      "Epoch [33/60], Step [200/480], Loss: 0.3183\n",
      "Epoch [33/60], Step [300/480], Loss: 0.2436\n",
      "Epoch [33/60], Step [400/480], Loss: 0.2248\n",
      "Epoch [34/60], Step [100/480], Loss: 0.2772\n",
      "Epoch [34/60], Step [200/480], Loss: 0.2013\n",
      "Epoch [34/60], Step [300/480], Loss: 0.1423\n",
      "Epoch [34/60], Step [400/480], Loss: 1.2798\n",
      "Epoch [35/60], Step [100/480], Loss: 0.2443\n",
      "Epoch [35/60], Step [200/480], Loss: 0.1093\n",
      "Epoch [35/60], Step [300/480], Loss: 0.0422\n",
      "Epoch [35/60], Step [400/480], Loss: 0.5470\n",
      "Epoch [36/60], Step [100/480], Loss: 0.9027\n",
      "Epoch [36/60], Step [200/480], Loss: 0.7521\n",
      "Epoch [36/60], Step [300/480], Loss: 0.6709\n",
      "Epoch [36/60], Step [400/480], Loss: 0.3978\n",
      "Epoch [37/60], Step [100/480], Loss: 0.0772\n",
      "Epoch [37/60], Step [200/480], Loss: 0.3911\n",
      "Epoch [37/60], Step [300/480], Loss: 0.4699\n",
      "Epoch [37/60], Step [400/480], Loss: 0.5146\n",
      "Epoch [38/60], Step [100/480], Loss: 0.0608\n",
      "Epoch [38/60], Step [200/480], Loss: 0.2363\n",
      "Epoch [38/60], Step [300/480], Loss: 0.5497\n",
      "Epoch [38/60], Step [400/480], Loss: 0.5157\n",
      "Epoch [39/60], Step [100/480], Loss: 0.3558\n",
      "Epoch [39/60], Step [200/480], Loss: 0.0572\n",
      "Epoch [39/60], Step [300/480], Loss: 0.2360\n",
      "Epoch [39/60], Step [400/480], Loss: 0.2979\n",
      "Epoch [40/60], Step [100/480], Loss: 0.4468\n",
      "Epoch [40/60], Step [200/480], Loss: 1.4326\n",
      "Epoch [40/60], Step [300/480], Loss: 0.3988\n",
      "Epoch [40/60], Step [400/480], Loss: 0.1882\n",
      "Epoch [41/60], Step [100/480], Loss: 0.1992\n",
      "Epoch [41/60], Step [200/480], Loss: 0.4798\n",
      "Epoch [41/60], Step [300/480], Loss: 0.2908\n",
      "Epoch [41/60], Step [400/480], Loss: 0.3099\n",
      "Epoch [42/60], Step [100/480], Loss: 0.2473\n",
      "Epoch [42/60], Step [200/480], Loss: 0.2691\n",
      "Epoch [42/60], Step [300/480], Loss: 0.1577\n",
      "Epoch [42/60], Step [400/480], Loss: 0.2330\n",
      "Epoch [43/60], Step [100/480], Loss: 0.1995\n",
      "Epoch [43/60], Step [200/480], Loss: 0.2120\n",
      "Epoch [43/60], Step [300/480], Loss: 0.0614\n",
      "Epoch [43/60], Step [400/480], Loss: 0.6040\n",
      "Epoch [44/60], Step [100/480], Loss: 0.1321\n",
      "Epoch [44/60], Step [200/480], Loss: 0.5190\n",
      "Epoch [44/60], Step [300/480], Loss: 0.5138\n",
      "Epoch [44/60], Step [400/480], Loss: 0.1593\n",
      "Epoch [45/60], Step [100/480], Loss: 0.3911\n",
      "Epoch [45/60], Step [200/480], Loss: 0.8415\n",
      "Epoch [45/60], Step [300/480], Loss: 0.1013\n",
      "Epoch [45/60], Step [400/480], Loss: 0.2933\n",
      "Epoch [46/60], Step [100/480], Loss: 0.0848\n",
      "Epoch [46/60], Step [200/480], Loss: 0.2111\n",
      "Epoch [46/60], Step [300/480], Loss: 0.0965\n",
      "Epoch [46/60], Step [400/480], Loss: 0.0934\n",
      "Epoch [47/60], Step [100/480], Loss: 0.0838\n",
      "Epoch [47/60], Step [200/480], Loss: 0.2213\n",
      "Epoch [47/60], Step [300/480], Loss: 0.4139\n",
      "Epoch [47/60], Step [400/480], Loss: 0.2459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/60], Step [100/480], Loss: 0.2894\n",
      "Epoch [48/60], Step [200/480], Loss: 0.4848\n",
      "Epoch [48/60], Step [300/480], Loss: 0.0883\n",
      "Epoch [48/60], Step [400/480], Loss: 0.1394\n",
      "Epoch [49/60], Step [100/480], Loss: 0.2683\n",
      "Epoch [49/60], Step [200/480], Loss: 0.3525\n",
      "Epoch [49/60], Step [300/480], Loss: 0.3126\n",
      "Epoch [49/60], Step [400/480], Loss: 0.3505\n",
      "Epoch [50/60], Step [100/480], Loss: 0.0153\n",
      "Epoch [50/60], Step [200/480], Loss: 0.4051\n",
      "Epoch [50/60], Step [300/480], Loss: 0.2377\n",
      "Epoch [50/60], Step [400/480], Loss: 0.2398\n",
      "Epoch [51/60], Step [100/480], Loss: 0.3488\n",
      "Epoch [51/60], Step [200/480], Loss: 0.1367\n",
      "Epoch [51/60], Step [300/480], Loss: 0.2963\n",
      "Epoch [51/60], Step [400/480], Loss: 0.2329\n",
      "Epoch [52/60], Step [100/480], Loss: 0.5665\n",
      "Epoch [52/60], Step [200/480], Loss: 0.0363\n",
      "Epoch [52/60], Step [300/480], Loss: 1.1696\n",
      "Epoch [52/60], Step [400/480], Loss: 0.1998\n",
      "Epoch [53/60], Step [100/480], Loss: 0.1116\n",
      "Epoch [53/60], Step [200/480], Loss: 0.1049\n",
      "Epoch [53/60], Step [300/480], Loss: 0.4543\n",
      "Epoch [53/60], Step [400/480], Loss: 0.4814\n",
      "Epoch [54/60], Step [100/480], Loss: 0.1644\n",
      "Epoch [54/60], Step [200/480], Loss: 0.5485\n",
      "Epoch [54/60], Step [300/480], Loss: 0.3483\n",
      "Epoch [54/60], Step [400/480], Loss: 0.0232\n",
      "Epoch [55/60], Step [100/480], Loss: 0.1264\n",
      "Epoch [55/60], Step [200/480], Loss: 0.2869\n",
      "Epoch [55/60], Step [300/480], Loss: 0.1561\n",
      "Epoch [55/60], Step [400/480], Loss: 0.0385\n",
      "Epoch [56/60], Step [100/480], Loss: 0.0426\n",
      "Epoch [56/60], Step [200/480], Loss: 0.4713\n",
      "Epoch [56/60], Step [300/480], Loss: 0.2074\n",
      "Epoch [56/60], Step [400/480], Loss: 0.7011\n",
      "Epoch [57/60], Step [100/480], Loss: 0.2503\n",
      "Epoch [57/60], Step [200/480], Loss: 0.1411\n",
      "Epoch [57/60], Step [300/480], Loss: 0.4427\n",
      "Epoch [57/60], Step [400/480], Loss: 0.0657\n",
      "Epoch [58/60], Step [100/480], Loss: 0.4680\n",
      "Epoch [58/60], Step [200/480], Loss: 0.0142\n",
      "Epoch [58/60], Step [300/480], Loss: 0.2440\n",
      "Epoch [58/60], Step [400/480], Loss: 0.0632\n",
      "Epoch [59/60], Step [100/480], Loss: 0.0144\n",
      "Epoch [59/60], Step [200/480], Loss: 0.0618\n",
      "Epoch [59/60], Step [300/480], Loss: 0.0223\n",
      "Epoch [59/60], Step [400/480], Loss: 0.1059\n",
      "Epoch [60/60], Step [100/480], Loss: 0.0864\n",
      "Epoch [60/60], Step [200/480], Loss: 0.5068\n",
      "Epoch [60/60], Step [300/480], Loss: 0.3452\n",
      "Epoch [60/60], Step [400/480], Loss: 0.0129\n",
      "Test Accuracy of the model on the test images: 80 %\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'np'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f76bb66debf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_lis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'np'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Tue May 15 11:25:41 2018\n",
    "\n",
    "@author: hugo\n",
    "\"\"\"\n",
    "from torch.autograd import Variable\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Transform\n",
    "data_transform = transforms.Compose(\n",
    "    [transforms.Grayscale(),\n",
    "     transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5,0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "#Train\n",
    "train_set = datasets.ImageFolder(root='16_clouds_easy',\n",
    "                                transform=data_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                             batch_size=8, shuffle=True,\n",
    "                                             num_workers=1)\n",
    "\n",
    "#Test\n",
    "test_set = datasets.ImageFolder(root='16_clouds_easy_test',\n",
    "                                transform=data_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                             batch_size=8,shuffle=False,\n",
    "                                             num_workers=1)\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "# Hyper-parameters\n",
    "sequence_length = 49\n",
    "input_size = 49\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 16\n",
    "batch_size = 8\n",
    "num_epochs = 60\n",
    "learning_rate = 0.003\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout = 0.3)\n",
    "        self.conv1 = nn.Conv2d(1,8,30)\n",
    "        self.conv2 = nn.Conv2d(1,8,30)\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        #self.drop = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc = nn.Linear(113 , 16)  # 2 for bidirection\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print('Input %s' % str(x.size()))\n",
    "\n",
    "        out = self.pool1(F.relu(self.conv1(x)))\n",
    "        #print('Convoluted %s' % str(x.size()))\n",
    "\n",
    "        # Init\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size) # 2 for bidirection \n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)\n",
    "        h0 = Variable(h0).cuda()\n",
    "        c0 = Variable(c0).cuda()\n",
    "        \n",
    "        out = out[:,-1,:,:]\n",
    "        #print('Resized for RNN %s' % str(x.size()))\n",
    "        \n",
    "        # LSTM forward\n",
    "        out, _ = self.rnn(out, (h0,c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "        #print('After RNN %s' % str(out.size()))\n",
    "        \n",
    "        out = out.unsqueeze(1)\n",
    "        #print(out.size())\n",
    "        out = self.pool2(F.relu(self.conv2(out)))\n",
    "        #print(out.size())\n",
    "        out = out[:,-1,:,:]\n",
    "        #Dropout\n",
    "        #out = self.drop(out)\n",
    "        #print(out.size())\n",
    "        \n",
    "        out = out[:,-1,:]\n",
    "        # LSTM output\n",
    "        out = self.fc(out)\n",
    "        #print('Reshaped for output %s \\n'%  str(out.size()))\n",
    "\n",
    "        return out\n",
    "\n",
    "model = BiRNN(input_size, hidden_size, num_layers, num_classes).cuda()\n",
    "print(model)\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "#plotting list\n",
    "loss_list = []\n",
    "time_list = []\n",
    "t = 0\n",
    "\n",
    "print(\"Start training\")\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for param in model.rnn.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "for epoch in range(num_epochs):\n",
    "     \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        freeze_layers\n",
    "        images = Variable(images).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.data[0]))\n",
    "            loss_list.append(loss.data[0])\n",
    "            time_list.append(t)\n",
    "            t+=1\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images).cuda()\n",
    "    labels = Variable(labels).cuda()\n",
    "\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += predicted.eq(labels.data).sum()\n",
    "\n",
    "print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import np\n",
    "plt.plot(np.asarray(time_list)[::4], np.asarray(loss_lis[::4]))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss for LSTM-CNN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW FOR THE PSYCHOPHYSICS\n",
    "import numpy as np\n",
    "import MotionClouds as mc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import imageio\n",
    "import random\n",
    "\n",
    "exp_info = 'model_ring_FROZEN_HIRES2'\n",
    "print (exp_info)\n",
    "\n",
    "downscale = 1\n",
    "fig_width = 21\n",
    "fx, fy, ft = mc.get_grids(mc.N_X/downscale, mc.N_Y/downscale, 1)\n",
    "\n",
    "# generates a cloud of given theta and b_theta\n",
    "def generate_random_cloud(theta, B_theta):\n",
    "    mc_i = mc.envelope_gabor(fx, fy, ft, V_X=0., V_Y=0.,\n",
    "                             B_V=0, theta=theta, B_theta=B_theta)\n",
    "    \n",
    "    im = mc.random_cloud(mc_i)\n",
    "    im = (mc.rectif(im) * 255).astype('uint8')\n",
    "    fname = './tmp/%s_%s.bmp' % (theta, B_theta)\n",
    "    imageio.imwrite(uri = fname,im = im[:, :, 0], format = 'bmp')\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psychopy import visual, core, event\n",
    "import MotionClouds as MC\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "\n",
    "files = glob.glob('./tmp/*')\n",
    "if not os.path.exists('./tmp'): #si le folder n'existe pas on le crée\n",
    "        os.makedirs('./tmp')\n",
    "        \n",
    "        \n",
    "test_length = 600\n",
    "MC1 = generate_random_cloud(np.pi/2, B_theta=np.pi/2)\n",
    "model = model.eval()\n",
    "\n",
    "ans_list = []\n",
    "N_B_thetas = 15\n",
    "B_thetas = np.pi*np.logspace(-6, -1, N_B_thetas, base=2)\n",
    "    \n",
    "std_theta = np.pi/6\n",
    "\n",
    "for trial in range(test_length):\n",
    "    theta = np.clip(std_theta *  np.random.randn(), -np.pi/4, np.pi/4)\n",
    "\n",
    "    # MC generation\n",
    "    B_theta = B_thetas[random.randint(0, N_B_thetas-1)]\n",
    "\n",
    "    MC1 = generate_random_cloud(np.pi/2, B_theta=B_theta)\n",
    "    MC2 = generate_random_cloud(np.pi/2 - theta, B_theta=B_theta)  # if shift = 2\n",
    "\n",
    "    varimgmc2 = data_transform(Image.open(MC2))\n",
    "    #varimgmc2 = data_transform(Image.open('16_clouds_easy/0.0/B0 1.40625.png'))\n",
    "    varimgmc2 = Variable(varimgmc2).cuda()\n",
    "    varimgmc2 = varimgmc2.unsqueeze(0)\n",
    "    \n",
    "    net_ans = model(varimgmc2)\n",
    "    pred = net_ans.data.max(1)[1] #prediction\n",
    "    #print(pred)\n",
    "    #correct = (np.sign(theta) > 0) and (pred[0]>8)\n",
    "    if np.sign(theta) > 0 and pred[0] > 8 :\n",
    "        correct = True\n",
    "    if np.sign(theta) < 0 and pred[0] <= 8:\n",
    "        correct = True\n",
    "    else :\n",
    "        correct = False\n",
    "    print('At trial ', trial, 'Angle=', '%3.3f' % (theta*180/np.pi), 'answer is ', pred[0], '(correct=', correct, '); bandwidth=', '%.3f' % (B_theta*180/np.pi))\n",
    "\n",
    "    # Output shape per trial is : trial number, shift direction, answered shift and b_theta\n",
    "    if pred[0] >8 :\n",
    "        ans_list.append([trial, theta, 'left', B_theta])\n",
    "    if pred[0] <= 8 :\n",
    "        ans_list.append([trial, theta, 'right', B_theta])\n",
    "\n",
    "#cleanup temp\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(ans_list, open('./psychophysics_data/Psy_discrim_final_%s.p' % exp_info, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "data_transform = transforms.Compose(\n",
    "    [transforms.Grayscale(),\n",
    "     transforms.Resize((128,128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5,0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "#train\n",
    "train_set = datasets.ImageFolder(root='16_clouds_easy',\n",
    "                                transform=data_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                             batch_size=4, shuffle=True,\n",
    "                                             num_workers=1, drop_last = True)\n",
    "\n",
    "#test\n",
    "test_set = datasets.ImageFolder(root='16_clouds_easy_test',\n",
    "                                transform=data_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                             batch_size=4,shuffle=True,\n",
    "                                             num_workers=1, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 20)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.fc3 = nn.Linear(17496,1000)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.outlayer = nn.Linear(1000,16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1) #reshape from conv to linear\n",
    "\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.outlayer(x)\n",
    "        return x\n",
    "        \n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "#unfreeze before starting\n",
    "layers_microwave(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #loss criterion\n",
    "optimizer = optim.SGD(params = model.parameters(),lr=0.001, momentum=0.9)\n",
    "epochs = 2 #nbr of epochs per layer\n",
    "model_size = 4 #nbr of layers\n",
    "\n",
    "print_interval = 50 #prints every p_i*4\n",
    "tempo = []\n",
    "acc = []\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Started training\")\n",
    "\n",
    "for epoch in range(epochs):  # nbr epochs\n",
    "    for batch_idx, (data, target) in enumerate(train_loader): #nbr batch,in,out\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        #On resize pour la sortie\n",
    "\n",
    "\n",
    "        #init l'entrainement\n",
    "        optimizer.zero_grad()\n",
    "        net_out = model(data)\n",
    "\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #afficher la progression\n",
    "        if batch_idx % print_interval == 0:\n",
    "            #le print statement le plus illisible du monde\n",
    "            print('Epoch: {} [{}/{} ({:.0f}%)]\\t\\tLoss: {:.6f}'.format(\n",
    "                    epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "    tempo.append(epoch)\n",
    "    acc.append(loss.data[0])\n",
    "\n",
    "\n",
    "print(\"Finished training in  %.3f seconds \" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "model = model.eval()\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "\n",
    "    net_out = model(data)\n",
    "    \n",
    "    #somme des pertes du batch\n",
    "    test_loss += criterion(net_out, target).data[0]\n",
    "    pred = net_out.data.max(1)[1] #prediction\n",
    "    correct += pred.eq(target.data).sum() #output du réseau\n",
    "\n",
    "test_loss /= len(test_loader.dataset) #loss = loss/length set\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convo psychophysique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#j'ai augmenté les B_theta et la longueur du test pour lisser les courbes\n",
    "exp_info = 'convo_model_HIRES'\n",
    "print (exp_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import MotionClouds as mc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import imageio\n",
    "import random\n",
    "\n",
    "downscale = 1\n",
    "fig_width = 21\n",
    "fx, fy, ft = mc.get_grids(mc.N_X/downscale, mc.N_Y/downscale, 1)\n",
    "\n",
    "# generates a cloud of given theta and b_theta\n",
    "def generate_random_cloud(theta, B_theta):\n",
    "    mc_i = mc.envelope_gabor(fx, fy, ft, V_X=0., V_Y=0.,\n",
    "                             B_V=0, theta=theta, B_theta=B_theta)\n",
    "    im = mc.random_cloud(mc_i)\n",
    "    im = (mc.rectif(im) * 255).astype('uint8')\n",
    "    fname = './tmp/%s_%s.png' % (theta, B_theta)\n",
    "    imageio.imwrite(fname, im[:, :, 0])\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psychopy import visual, core, event\n",
    "import MotionClouds as MC\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimage\n",
    "from PIL import Image\n",
    "\n",
    "test_length = 600\n",
    "MC1 = generate_random_cloud(np.pi/2, B_theta=np.pi/2)\n",
    "model = model.eval()\n",
    "\n",
    "\n",
    "ans_list = []\n",
    "N_B_thetas = 15\n",
    "B_thetas = np.pi*np.logspace(-6, -1, N_B_thetas, base=2)\n",
    "    \n",
    "std_theta = np.pi/6\n",
    "\n",
    "for trial in range(test_length):\n",
    "    theta = np.clip(std_theta *  np.random.randn(), -np.pi/4, np.pi/4)\n",
    "\n",
    "    # MC generation\n",
    "    B_theta = B_thetas[random.randint(0, N_B_thetas-1)]\n",
    "\n",
    "    MC1 = generate_random_cloud(np.pi/2, B_theta=B_theta)\n",
    "    MC2 = generate_random_cloud(np.pi/2 - theta, B_theta=B_theta)  # if shift = 2\n",
    "\n",
    "    varimgmc2 = data_transform(Image.open(MC2))\n",
    "    #varimgmc2 = data_transform(Image.open('16_clouds_easy/0.0/B0 1.40625.png'))\n",
    "    varimgmc2 = Variable(varimgmc2)\n",
    "    varimgmc2 = varimgmc2.unsqueeze(0)\n",
    "    \n",
    "    net_ans = model(varimgmc2)\n",
    "    pred = net_ans.data.max(1)[1] #prediction\n",
    "    #print(pred)\n",
    "    #correct = (np.sign(theta) > 0) and (pred[0]>8)\n",
    "    if np.sign(theta) > 0 and pred[0] > 8 :\n",
    "        correct = True\n",
    "    if np.sign(theta) < 0 and pred[0] <= 8:\n",
    "        correct = True\n",
    "    else :\n",
    "        correct = False\n",
    "    print('At trial ', trial, 'Angle=', '%3.3f' % (theta*180/np.pi), 'answer is ', pred[0], '(correct=', correct, '); bandwidth=', '%.3f' % (B_theta*180/np.pi))\n",
    "\n",
    "    # Output shape per trial is : trial number, shift direction, answered shift and b_theta\n",
    "    if pred[0] >8 :\n",
    "        ans_list.append([trial, theta, 'left', B_theta])\n",
    "    if pred[0] <= 8 :\n",
    "        ans_list.append([trial, theta, 'right', B_theta])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(ans_list, open('./psychophysics_data/Psy_discrim_final_%s.p' % exp_info, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
