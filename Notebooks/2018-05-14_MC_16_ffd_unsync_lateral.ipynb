{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018-05-14 - Test feedforward, unsync, lateral avec 16 thetas \n",
    "## Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images en 64x64 parce que le 32x32 est trop aliasé pour que le network puisse apprendre :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "data_transform = transforms.Compose(\n",
    "    [transforms.Grayscale(),\n",
    "     transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5,0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "train_set = datasets.ImageFolder(root='16_clouds_easy',\n",
    "                                transform=data_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                             batch_size=16, shuffle=True,\n",
    "                                             num_workers=1)\n",
    "\n",
    "test_set = datasets.ImageFolder(root='16_clouds_easy_test',\n",
    "                                transform=data_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                             batch_size=16,shuffle=False,\n",
    "                                             num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche des images du set importé pour vérifier que tout a bien marché :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# pour montrer une image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5    #de-normaliser\n",
    "    npimg = img.numpy()    #convertir en array\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# on loop sur un batch\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réseau à trois couches :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(64*64,1000)\n",
    "            self.fc2 = nn.Linear(1000, 200)\n",
    "            self.fc3 = nn.Linear(200, 16)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return F.log_softmax(input=x)\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et l'optimiseur, toujours en SGD mais avec un learning rate 10 fois plus grand. Avec NLLL comme critère, on a rajouté une couche de softmax en sortie pour obtenir des log-proba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraine :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "print(\"Started training\")\n",
    "\n",
    "epochs = 50\n",
    "print_interval = 50 #prints every p_i*4\n",
    "tempo = []\n",
    "acc = []\n",
    "\n",
    "for epoch in range(epochs):  # nbr epochs\n",
    "    for batch_idx, (data, target) in enumerate(train_loader): #nbr batch,in,out\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        #On resize pour la sortie\n",
    "        data = data.view(-1, 64*64)\n",
    "\n",
    "        #init l'entrainement\n",
    "        optimizer.zero_grad()\n",
    "        net_out = model(data)\n",
    "\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #afficher la progression\n",
    "        if batch_idx % print_interval == 0:\n",
    "            #le print statement le plus illisible du monde\n",
    "            print('Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "    tempo.append(epoch)\n",
    "    acc.append(loss.data[0])\n",
    "    \n",
    "print(\"Finished training in  %.3f seconds \" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et maintenant on teste :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    \n",
    "    #rescale\n",
    "    data = data.view(-1, 64 * 64)\n",
    "    net_out = model(data)\n",
    "    \n",
    "    #somme des pertes du batch\n",
    "    test_loss += criterion(net_out, target).data[0]\n",
    "    pred = net_out.data.max(1)[1] #prediction\n",
    "    correct += pred.eq(target.data).sum() #output du réseau\n",
    "\n",
    "test_loss /= len(test_loader.dataset) #loss = loss/length set\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sauvegarde le modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"MODEL_trainEASY_pytorchMC16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joli plot du loss en fonction de l'epoch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(tempo, acc)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss variation - Easy training')\n",
    "plt.savefig('Loss_easytraining.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsync learning\n",
    "Layer par layer donc, comme lors de l'ajout du L2 sur CHAMP :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "data_transform = transforms.Compose(\n",
    "    [transforms.Grayscale(),\n",
    "     transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5,0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "train_set = datasets.ImageFolder(root='16_clouds_easy',\n",
    "                                transform=data_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                             batch_size=16, shuffle=True,\n",
    "                                             num_workers=1)\n",
    "\n",
    "test_set = datasets.ImageFolder(root='16_clouds_easy_test',\n",
    "                                transform=data_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                             batch_size=16,shuffle=False,\n",
    "                                             num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le même network qu'en forward classique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(64*64,1000)\n",
    "            self.fc2 = nn.Linear(1000, 200)\n",
    "            self.fc3 = nn.Linear(200, 16)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return F.log_softmax(input=x)\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On modifie les flags du réseau pour n'entrainer qu'un seul layer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze layers after and including freezing_layer+1 (layers start at 0)\n",
    "def freeze_layers(freezing_layer, nn_model = model) :\n",
    "    for count,child in enumerate(model.children()) :\n",
    "        if count < freezing_layer+1 : #to freeze at iteration 1\n",
    "            print(\"Layer no. %s -- %s -- NOT FROZEN\"% (count,child))\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        else :\n",
    "            print(\"Layer no. %s -- %s -- FROZEN\"%(count, child))\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "#freeze all the layers except the unfrozen one\n",
    "def freeze_all_layers(unfrozen_layer, nn_model = model) :\n",
    "    for count,child in enumerate(model.children()) :\n",
    "        if count == unfrozen_layer :\n",
    "            print(\"Layer no. %s -- %s -- NOT FROZEN\"% (count,child))\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        else :\n",
    "            print(\"Layer no. %s -- %s -- FROZEN\"%(count, child))\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "#unfreeze every layers\n",
    "def layers_microwave(nn_model = model) :\n",
    "    for count,child in enumerate(model.children()) :\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training\n",
      "Layer no. 0 -- Linear(in_features=4096, out_features=1000, bias=True) -- NOT FROZEN\n",
      "Layer no. 1 -- Linear(in_features=1000, out_features=200, bias=True) -- FROZEN\n",
      "Layer no. 2 -- Linear(in_features=200, out_features=16, bias=True) -- FROZEN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [0/1920 (0%)]\t\tLoss: 2.782913\n",
      "Epoch: 1 [800/1920 (42%)]\t\tLoss: 2.777709\n",
      "Epoch: 1 [1600/1920 (83%)]\t\tLoss: 2.783829\n",
      "Epoch: 2 [0/1920 (0%)]\t\tLoss: 2.756960\n",
      "Epoch: 2 [800/1920 (42%)]\t\tLoss: 2.768983\n",
      "Epoch: 2 [1600/1920 (83%)]\t\tLoss: 2.773184\n",
      "Epoch: 3 [0/1920 (0%)]\t\tLoss: 2.749019\n",
      "Epoch: 3 [800/1920 (42%)]\t\tLoss: 2.735907\n",
      "Epoch: 3 [1600/1920 (83%)]\t\tLoss: 2.749079\n",
      "Epoch: 4 [0/1920 (0%)]\t\tLoss: 2.732347\n",
      "Epoch: 4 [800/1920 (42%)]\t\tLoss: 2.725827\n",
      "Epoch: 4 [1600/1920 (83%)]\t\tLoss: 2.762542\n",
      "Epoch: 5 [0/1920 (0%)]\t\tLoss: 2.721490\n",
      "Epoch: 5 [800/1920 (42%)]\t\tLoss: 2.716129\n",
      "Epoch: 5 [1600/1920 (83%)]\t\tLoss: 2.726128\n",
      "Epoch: 6 [0/1920 (0%)]\t\tLoss: 2.728941\n",
      "Epoch: 6 [800/1920 (42%)]\t\tLoss: 2.729384\n",
      "Epoch: 6 [1600/1920 (83%)]\t\tLoss: 2.706786\n",
      "Epoch: 7 [0/1920 (0%)]\t\tLoss: 2.704934\n",
      "Epoch: 7 [800/1920 (42%)]\t\tLoss: 2.703886\n",
      "Epoch: 7 [1600/1920 (83%)]\t\tLoss: 2.688040\n",
      "Epoch: 8 [0/1920 (0%)]\t\tLoss: 2.698377\n",
      "Epoch: 8 [800/1920 (42%)]\t\tLoss: 2.682473\n",
      "Epoch: 8 [1600/1920 (83%)]\t\tLoss: 2.686484\n",
      "Epoch: 9 [0/1920 (0%)]\t\tLoss: 2.677003\n",
      "Epoch: 9 [800/1920 (42%)]\t\tLoss: 2.673625\n",
      "Epoch: 9 [1600/1920 (83%)]\t\tLoss: 2.671539\n",
      "Epoch: 10 [0/1920 (0%)]\t\tLoss: 2.674862\n",
      "Epoch: 10 [800/1920 (42%)]\t\tLoss: 2.667562\n",
      "Epoch: 10 [1600/1920 (83%)]\t\tLoss: 2.656614\n",
      "Epoch: 11 [0/1920 (0%)]\t\tLoss: 2.643037\n",
      "Epoch: 11 [800/1920 (42%)]\t\tLoss: 2.642025\n",
      "Epoch: 11 [1600/1920 (83%)]\t\tLoss: 2.663430\n",
      "Epoch: 12 [0/1920 (0%)]\t\tLoss: 2.604378\n",
      "Epoch: 12 [800/1920 (42%)]\t\tLoss: 2.636468\n",
      "Epoch: 12 [1600/1920 (83%)]\t\tLoss: 2.613560\n",
      "Epoch: 13 [0/1920 (0%)]\t\tLoss: 2.626127\n",
      "Epoch: 13 [800/1920 (42%)]\t\tLoss: 2.645975\n",
      "Epoch: 13 [1600/1920 (83%)]\t\tLoss: 2.617255\n",
      "Epoch: 14 [0/1920 (0%)]\t\tLoss: 2.606662\n",
      "Epoch: 14 [800/1920 (42%)]\t\tLoss: 2.605465\n",
      "Epoch: 14 [1600/1920 (83%)]\t\tLoss: 2.587394\n",
      "Epoch: 15 [0/1920 (0%)]\t\tLoss: 2.602490\n",
      "Epoch: 15 [800/1920 (42%)]\t\tLoss: 2.577780\n",
      "Epoch: 15 [1600/1920 (83%)]\t\tLoss: 2.627937\n",
      "Epoch: 16 [0/1920 (0%)]\t\tLoss: 2.598933\n",
      "Epoch: 16 [800/1920 (42%)]\t\tLoss: 2.602380\n",
      "Epoch: 16 [1600/1920 (83%)]\t\tLoss: 2.597241\n",
      "Epoch: 17 [0/1920 (0%)]\t\tLoss: 2.531921\n",
      "Epoch: 17 [800/1920 (42%)]\t\tLoss: 2.577438\n",
      "Epoch: 17 [1600/1920 (83%)]\t\tLoss: 2.507775\n",
      "Epoch: 18 [0/1920 (0%)]\t\tLoss: 2.535637\n",
      "Epoch: 18 [800/1920 (42%)]\t\tLoss: 2.537760\n",
      "Epoch: 18 [1600/1920 (83%)]\t\tLoss: 2.534300\n",
      "Epoch: 19 [0/1920 (0%)]\t\tLoss: 2.547305\n",
      "Epoch: 19 [800/1920 (42%)]\t\tLoss: 2.507311\n",
      "Epoch: 19 [1600/1920 (83%)]\t\tLoss: 2.547102\n",
      "Epoch: 20 [0/1920 (0%)]\t\tLoss: 2.468942\n",
      "Epoch: 20 [800/1920 (42%)]\t\tLoss: 2.509967\n",
      "Epoch: 20 [1600/1920 (83%)]\t\tLoss: 2.565678\n",
      "Layer no. 0 -- Linear(in_features=4096, out_features=1000, bias=True) -- FROZEN\n",
      "Layer no. 1 -- Linear(in_features=1000, out_features=200, bias=True) -- NOT FROZEN\n",
      "Layer no. 2 -- Linear(in_features=200, out_features=16, bias=True) -- FROZEN\n",
      "Epoch: 1 [0/1920 (0%)]\t\tLoss: 2.473586\n",
      "Epoch: 1 [800/1920 (42%)]\t\tLoss: 2.443475\n",
      "Epoch: 1 [1600/1920 (83%)]\t\tLoss: 2.498925\n",
      "Epoch: 2 [0/1920 (0%)]\t\tLoss: 2.264397\n",
      "Epoch: 2 [800/1920 (42%)]\t\tLoss: 2.136601\n",
      "Epoch: 2 [1600/1920 (83%)]\t\tLoss: 1.963356\n",
      "Epoch: 3 [0/1920 (0%)]\t\tLoss: 2.261133\n",
      "Epoch: 3 [800/1920 (42%)]\t\tLoss: 2.096589\n",
      "Epoch: 3 [1600/1920 (83%)]\t\tLoss: 1.948624\n",
      "Epoch: 4 [0/1920 (0%)]\t\tLoss: 1.889488\n",
      "Epoch: 4 [800/1920 (42%)]\t\tLoss: 1.847646\n",
      "Epoch: 4 [1600/1920 (83%)]\t\tLoss: 2.079125\n",
      "Epoch: 5 [0/1920 (0%)]\t\tLoss: 1.687170\n",
      "Epoch: 5 [800/1920 (42%)]\t\tLoss: 1.754744\n",
      "Epoch: 5 [1600/1920 (83%)]\t\tLoss: 1.687008\n",
      "Epoch: 6 [0/1920 (0%)]\t\tLoss: 1.561731\n",
      "Epoch: 6 [800/1920 (42%)]\t\tLoss: 1.521260\n",
      "Epoch: 6 [1600/1920 (83%)]\t\tLoss: 1.632355\n",
      "Epoch: 7 [0/1920 (0%)]\t\tLoss: 1.547932\n",
      "Epoch: 7 [800/1920 (42%)]\t\tLoss: 1.538317\n",
      "Epoch: 7 [1600/1920 (83%)]\t\tLoss: 1.460476\n",
      "Epoch: 8 [0/1920 (0%)]\t\tLoss: 1.511873\n",
      "Epoch: 8 [800/1920 (42%)]\t\tLoss: 1.527930\n",
      "Epoch: 8 [1600/1920 (83%)]\t\tLoss: 1.798103\n",
      "Epoch: 9 [0/1920 (0%)]\t\tLoss: 1.287401\n",
      "Epoch: 9 [800/1920 (42%)]\t\tLoss: 1.356012\n",
      "Epoch: 9 [1600/1920 (83%)]\t\tLoss: 1.318569\n",
      "Epoch: 10 [0/1920 (0%)]\t\tLoss: 1.356161\n",
      "Epoch: 10 [800/1920 (42%)]\t\tLoss: 1.077696\n",
      "Epoch: 10 [1600/1920 (83%)]\t\tLoss: 0.984056\n",
      "Epoch: 11 [0/1920 (0%)]\t\tLoss: 1.496051\n",
      "Epoch: 11 [800/1920 (42%)]\t\tLoss: 1.032646\n",
      "Epoch: 11 [1600/1920 (83%)]\t\tLoss: 1.015819\n",
      "Epoch: 12 [0/1920 (0%)]\t\tLoss: 1.334397\n",
      "Epoch: 12 [800/1920 (42%)]\t\tLoss: 1.030288\n",
      "Epoch: 12 [1600/1920 (83%)]\t\tLoss: 0.808098\n",
      "Epoch: 13 [0/1920 (0%)]\t\tLoss: 1.002220\n",
      "Epoch: 13 [800/1920 (42%)]\t\tLoss: 0.922693\n",
      "Epoch: 13 [1600/1920 (83%)]\t\tLoss: 0.939855\n",
      "Epoch: 14 [0/1920 (0%)]\t\tLoss: 1.155008\n",
      "Epoch: 14 [800/1920 (42%)]\t\tLoss: 1.463794\n",
      "Epoch: 14 [1600/1920 (83%)]\t\tLoss: 1.123462\n",
      "Epoch: 15 [0/1920 (0%)]\t\tLoss: 0.838608\n",
      "Epoch: 15 [800/1920 (42%)]\t\tLoss: 0.954767\n",
      "Epoch: 15 [1600/1920 (83%)]\t\tLoss: 0.887190\n",
      "Epoch: 16 [0/1920 (0%)]\t\tLoss: 1.068288\n",
      "Epoch: 16 [800/1920 (42%)]\t\tLoss: 1.014319\n",
      "Epoch: 16 [1600/1920 (83%)]\t\tLoss: 1.276996\n",
      "Epoch: 17 [0/1920 (0%)]\t\tLoss: 0.881671\n",
      "Epoch: 17 [800/1920 (42%)]\t\tLoss: 0.819269\n",
      "Epoch: 17 [1600/1920 (83%)]\t\tLoss: 1.060247\n",
      "Epoch: 18 [0/1920 (0%)]\t\tLoss: 0.610664\n",
      "Epoch: 18 [800/1920 (42%)]\t\tLoss: 0.732928\n",
      "Epoch: 18 [1600/1920 (83%)]\t\tLoss: 0.897720\n",
      "Epoch: 19 [0/1920 (0%)]\t\tLoss: 0.998083\n",
      "Epoch: 19 [800/1920 (42%)]\t\tLoss: 0.604215\n",
      "Epoch: 19 [1600/1920 (83%)]\t\tLoss: 0.809743\n",
      "Epoch: 20 [0/1920 (0%)]\t\tLoss: 0.680624\n",
      "Epoch: 20 [800/1920 (42%)]\t\tLoss: 0.408799\n",
      "Epoch: 20 [1600/1920 (83%)]\t\tLoss: 0.942726\n",
      "Layer no. 0 -- Linear(in_features=4096, out_features=1000, bias=True) -- FROZEN\n",
      "Layer no. 1 -- Linear(in_features=1000, out_features=200, bias=True) -- FROZEN\n",
      "Layer no. 2 -- Linear(in_features=200, out_features=16, bias=True) -- NOT FROZEN\n",
      "Epoch: 1 [0/1920 (0%)]\t\tLoss: 0.791971\n",
      "Epoch: 1 [800/1920 (42%)]\t\tLoss: 0.552922\n",
      "Epoch: 1 [1600/1920 (83%)]\t\tLoss: 0.518230\n",
      "Epoch: 2 [0/1920 (0%)]\t\tLoss: 0.274231\n",
      "Epoch: 2 [800/1920 (42%)]\t\tLoss: 0.395611\n",
      "Epoch: 2 [1600/1920 (83%)]\t\tLoss: 0.519734\n",
      "Epoch: 3 [0/1920 (0%)]\t\tLoss: 0.086601\n",
      "Epoch: 3 [800/1920 (42%)]\t\tLoss: 0.299500\n",
      "Epoch: 3 [1600/1920 (83%)]\t\tLoss: 0.703278\n",
      "Epoch: 4 [0/1920 (0%)]\t\tLoss: 0.224026\n",
      "Epoch: 4 [800/1920 (42%)]\t\tLoss: 0.158038\n",
      "Epoch: 4 [1600/1920 (83%)]\t\tLoss: 0.167501\n",
      "Epoch: 5 [0/1920 (0%)]\t\tLoss: 0.179818\n",
      "Epoch: 5 [800/1920 (42%)]\t\tLoss: 0.703303\n",
      "Epoch: 5 [1600/1920 (83%)]\t\tLoss: 0.046753\n",
      "Epoch: 6 [0/1920 (0%)]\t\tLoss: 0.027782\n",
      "Epoch: 6 [800/1920 (42%)]\t\tLoss: 0.223273\n",
      "Epoch: 6 [1600/1920 (83%)]\t\tLoss: 0.365325\n",
      "Epoch: 7 [0/1920 (0%)]\t\tLoss: 0.367726\n",
      "Epoch: 7 [800/1920 (42%)]\t\tLoss: 0.189122\n",
      "Epoch: 7 [1600/1920 (83%)]\t\tLoss: 0.173252\n",
      "Epoch: 8 [0/1920 (0%)]\t\tLoss: 0.050779\n",
      "Epoch: 8 [800/1920 (42%)]\t\tLoss: 0.070257\n",
      "Epoch: 8 [1600/1920 (83%)]\t\tLoss: 0.117760\n",
      "Epoch: 9 [0/1920 (0%)]\t\tLoss: 0.216979\n",
      "Epoch: 9 [800/1920 (42%)]\t\tLoss: 0.066160\n",
      "Epoch: 9 [1600/1920 (83%)]\t\tLoss: 0.065190\n",
      "Epoch: 10 [0/1920 (0%)]\t\tLoss: 0.474071\n",
      "Epoch: 10 [800/1920 (42%)]\t\tLoss: 0.114043\n",
      "Epoch: 10 [1600/1920 (83%)]\t\tLoss: 0.054210\n",
      "Epoch: 11 [0/1920 (0%)]\t\tLoss: 0.140618\n",
      "Epoch: 11 [800/1920 (42%)]\t\tLoss: 0.057781\n",
      "Epoch: 11 [1600/1920 (83%)]\t\tLoss: 0.300143\n",
      "Epoch: 12 [0/1920 (0%)]\t\tLoss: 0.080367\n",
      "Epoch: 12 [800/1920 (42%)]\t\tLoss: 0.614430\n",
      "Epoch: 12 [1600/1920 (83%)]\t\tLoss: 0.341592\n",
      "Epoch: 13 [0/1920 (0%)]\t\tLoss: 0.036702\n",
      "Epoch: 13 [800/1920 (42%)]\t\tLoss: 0.189901\n",
      "Epoch: 13 [1600/1920 (83%)]\t\tLoss: 0.547588\n",
      "Epoch: 14 [0/1920 (0%)]\t\tLoss: 0.112210\n",
      "Epoch: 14 [800/1920 (42%)]\t\tLoss: 0.130856\n",
      "Epoch: 14 [1600/1920 (83%)]\t\tLoss: 0.185165\n",
      "Epoch: 15 [0/1920 (0%)]\t\tLoss: 0.089467\n",
      "Epoch: 15 [800/1920 (42%)]\t\tLoss: 0.126070\n",
      "Epoch: 15 [1600/1920 (83%)]\t\tLoss: 0.045132\n",
      "Epoch: 16 [0/1920 (0%)]\t\tLoss: 0.565779\n",
      "Epoch: 16 [800/1920 (42%)]\t\tLoss: 0.094658\n",
      "Epoch: 16 [1600/1920 (83%)]\t\tLoss: 0.018483\n",
      "Epoch: 17 [0/1920 (0%)]\t\tLoss: 0.305658\n",
      "Epoch: 17 [800/1920 (42%)]\t\tLoss: 0.179354\n",
      "Epoch: 17 [1600/1920 (83%)]\t\tLoss: 0.106821\n",
      "Epoch: 18 [0/1920 (0%)]\t\tLoss: 0.114275\n",
      "Epoch: 18 [800/1920 (42%)]\t\tLoss: 0.601153\n",
      "Epoch: 18 [1600/1920 (83%)]\t\tLoss: 0.247272\n",
      "Epoch: 19 [0/1920 (0%)]\t\tLoss: 0.184332\n",
      "Epoch: 19 [800/1920 (42%)]\t\tLoss: 0.204423\n",
      "Epoch: 19 [1600/1920 (83%)]\t\tLoss: 0.144630\n",
      "Epoch: 20 [0/1920 (0%)]\t\tLoss: 0.467260\n",
      "Epoch: 20 [800/1920 (42%)]\t\tLoss: 0.392727\n",
      "Epoch: 20 [1600/1920 (83%)]\t\tLoss: 0.016574\n",
      "Finished training in  683.248 seconds \n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "#unfreeze before starting\n",
    "layers_microwave(model)\n",
    "\n",
    "criterion = nn.NLLLoss() #loss criterion\n",
    "epochs = 20 #nbr of epochs per layer\n",
    "model_size = 3 #nbr of layers\n",
    "\n",
    "print_interval = 50 #prints every p_i*4\n",
    "tempo = []\n",
    "acc = []\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Started training\")\n",
    "\n",
    "\n",
    "#################\n",
    "#Loop over layers\n",
    "################\n",
    "for layer in range(model_size) :\n",
    "    freeze_all_layers(layer, model)\n",
    "    optimizer = optim.SGD(filter(lambda p : p.requires_grad, model.parameters()), #optimize only on requires_grad layers\n",
    "                      lr=0.01, momentum=0.9)\n",
    "    \n",
    "    #################\n",
    "    #Loop over epochs\n",
    "    ################\n",
    "    for epoch in range(epochs):  # nbr epochs\n",
    "        for batch_idx, (data, target) in enumerate(train_loader): #nbr batch,in,out\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "            #On resize pour la sortie\n",
    "            data = data.view(-1, 64*64)\n",
    "\n",
    "            #init l'entrainement\n",
    "            optimizer.zero_grad()\n",
    "            net_out = model(data)\n",
    "\n",
    "            loss = criterion(net_out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #afficher la progression\n",
    "            if batch_idx % print_interval == 0:\n",
    "                #le print statement le plus illisible du monde\n",
    "                print('Epoch: {} [{}/{} ({:.0f}%)]\\t\\tLoss: {:.6f}'.format(\n",
    "                        epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "        tempo.append(epoch)\n",
    "        acc.append(loss.data[0])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Finished training in  %.3f seconds \" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2125, Accuracy: 73/192 (38%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    \n",
    "    #rescale\n",
    "    data = data.view(-1, 64 * 64)\n",
    "    net_out = model(data)\n",
    "    \n",
    "    #somme des pertes du batch\n",
    "    test_loss += criterion(net_out, target).data[0]\n",
    "    pred = net_out.data.max(1)[1] #prediction\n",
    "    correct += pred.eq(target.data).sum() #output du réseau\n",
    "\n",
    "test_loss /= len(test_loader.dataset) #loss = loss/length set\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc6e1747438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#joli plot du loss en fonction de l'epoch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(tempo2, acc)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss variation - Easy training')\n",
    "plt.savefig('mc16easy_unsync.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lateral "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "data_transform = transforms.Compose(\n",
    "    [transforms.Grayscale(),\n",
    "     transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5,0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "train_set = datasets.ImageFolder(root='16_clouds_easy',\n",
    "                                transform=data_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                             batch_size=16, shuffle=True,\n",
    "                                             num_workers=1)\n",
    "\n",
    "test_set = datasets.ImageFolder(root='16_clouds_easy_test',\n",
    "                                transform=data_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                             batch_size=16,shuffle=False,\n",
    "                                             num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  (rec): RNN(1000, 200, num_layers=2, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            #forward\n",
    "            self.fc1 = nn.Linear(64*64,1000)\n",
    "            self.rec = nn.RNN(1000, hidden_size = 200,\n",
    "                             num_layers=2, nonlinearity='relu',\n",
    "                             batch_first = True)\n",
    "\n",
    "        \n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.rec(x)\n",
    "            return F.log_softmax(input=x)\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input must have 3 dimensions, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d942f49f7e47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#init l'entrainement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mnet_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-df229a9a7e65>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mflat_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         func = self._backend.RNN(\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    135\u001b[0m             raise RuntimeError(\n\u001b[1;32m    136\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[0;32m--> 137\u001b[0;31m                     expected_input_dim, input.dim()))\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             raise RuntimeError(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 2"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "print(\"Started training\")\n",
    "\n",
    "epochs = 50\n",
    "print_interval = 50 #prints every p_i*4\n",
    "tempo = []\n",
    "acc = []\n",
    "\n",
    "for epoch in range(epochs):  # nbr epochs\n",
    "    for batch_idx, (data, target) in enumerate(train_loader): #nbr batch,in,out\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        #On resize pour la sortie\n",
    "        data = data.view(-1, 64*64)\n",
    "\n",
    "        #init l'entrainement\n",
    "        optimizer.zero_grad()\n",
    "        net_out = model(data)\n",
    "\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #afficher la progression\n",
    "        if batch_idx % print_interval == 0:\n",
    "            #le print statement le plus illisible du monde\n",
    "            print('Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "    tempo.append(epoch)\n",
    "    acc.append(loss.data[0])\n",
    "    \n",
    "print(\"Finished training in  %.3f seconds \" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
