{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018-04-26 - Améliorer le réseau\n",
    "On reprend le notebook de la veille et un améliore les performances, avec notamment une séparation correcte des batchs de training et de test. Le réseau convolutionné ne convergeait pas, donc on le rétrécit en un réseau de 3 couches linéaires.\n",
    "\n",
    "On importe les données :\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "data_transform = transforms.Compose(\n",
    "    [transforms.Grayscale(),\n",
    "     transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5,0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "train_set = datasets.ImageFolder(root='clouds_easy',\n",
    "                                transform=data_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                             batch_size=4, shuffle=True,\n",
    "                                             num_workers=1)\n",
    "\n",
    "test_set = datasets.ImageFolder(root='clouds_easy_test',\n",
    "                                transform=data_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                             batch_size=4,shuffle=False,\n",
    "                                             num_workers=1)\n",
    "#les 4 thetas qu'on essaie d'apprendre\n",
    "cloud_classes = ('0', 'pi/4', 'pi/2', '3pi/4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche des images du set importé pour vérifier que tout a bien marché :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi/4          3pi/4          pi/2          0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# pour montrer une image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5    #de-normaliser\n",
    "    npimg = img.numpy()    #convertir en array\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# on loop sur un batch\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('          '.join('%s' % cloud_classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et maintenant on défini le réseau :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=1024, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(32 * 32, 200)\n",
    "            self.fc2 = nn.Linear(200, 200)\n",
    "            self.fc3 = nn.Linear(200, 4)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return F.log_softmax(input=x)\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et l'optimiseur, toujours en SGD mais avec un learning rate 10 fois plus grand. Avec NLLL comme critère, on a rajouté une couche de softmax en sortie pour obtenir des log-proba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraine :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training\n",
      "Epoch: 1 [0/2000 (0%)]\tLoss: 1.420514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [200/2000 (10%)]\tLoss: 1.402425\n",
      "Epoch: 1 [400/2000 (20%)]\tLoss: 1.386590\n",
      "Epoch: 1 [600/2000 (30%)]\tLoss: 1.280759\n",
      "Epoch: 1 [800/2000 (40%)]\tLoss: 1.464157\n",
      "Epoch: 1 [1000/2000 (50%)]\tLoss: 1.438900\n",
      "Epoch: 1 [1200/2000 (60%)]\tLoss: 1.395345\n",
      "Epoch: 1 [1400/2000 (70%)]\tLoss: 1.331726\n",
      "Epoch: 1 [1600/2000 (80%)]\tLoss: 1.385785\n",
      "Epoch: 1 [1800/2000 (90%)]\tLoss: 1.506883\n",
      "Epoch: 2 [0/2000 (0%)]\tLoss: 1.388134\n",
      "Epoch: 2 [200/2000 (10%)]\tLoss: 1.350660\n",
      "Epoch: 2 [400/2000 (20%)]\tLoss: 1.381277\n",
      "Epoch: 2 [600/2000 (30%)]\tLoss: 1.394785\n",
      "Epoch: 2 [800/2000 (40%)]\tLoss: 1.431708\n",
      "Epoch: 2 [1000/2000 (50%)]\tLoss: 1.363331\n",
      "Epoch: 2 [1200/2000 (60%)]\tLoss: 1.305024\n",
      "Epoch: 2 [1400/2000 (70%)]\tLoss: 1.393118\n",
      "Epoch: 2 [1600/2000 (80%)]\tLoss: 1.460998\n",
      "Epoch: 2 [1800/2000 (90%)]\tLoss: 1.329283\n",
      "Epoch: 3 [0/2000 (0%)]\tLoss: 1.433673\n",
      "Epoch: 3 [200/2000 (10%)]\tLoss: 1.433547\n",
      "Epoch: 3 [400/2000 (20%)]\tLoss: 1.233086\n",
      "Epoch: 3 [600/2000 (30%)]\tLoss: 1.285923\n",
      "Epoch: 3 [800/2000 (40%)]\tLoss: 1.413773\n",
      "Epoch: 3 [1000/2000 (50%)]\tLoss: 1.374312\n",
      "Epoch: 3 [1200/2000 (60%)]\tLoss: 1.400056\n",
      "Epoch: 3 [1400/2000 (70%)]\tLoss: 1.331931\n",
      "Epoch: 3 [1600/2000 (80%)]\tLoss: 1.343240\n",
      "Epoch: 3 [1800/2000 (90%)]\tLoss: 1.336136\n",
      "Epoch: 4 [0/2000 (0%)]\tLoss: 1.390447\n",
      "Epoch: 4 [200/2000 (10%)]\tLoss: 1.275151\n",
      "Epoch: 4 [400/2000 (20%)]\tLoss: 1.224658\n",
      "Epoch: 4 [600/2000 (30%)]\tLoss: 1.227585\n",
      "Epoch: 4 [800/2000 (40%)]\tLoss: 1.102113\n",
      "Epoch: 4 [1000/2000 (50%)]\tLoss: 1.049022\n",
      "Epoch: 4 [1200/2000 (60%)]\tLoss: 1.264000\n",
      "Epoch: 4 [1400/2000 (70%)]\tLoss: 1.244365\n",
      "Epoch: 4 [1600/2000 (80%)]\tLoss: 1.142078\n",
      "Epoch: 4 [1800/2000 (90%)]\tLoss: 1.079993\n",
      "Epoch: 5 [0/2000 (0%)]\tLoss: 1.184425\n",
      "Epoch: 5 [200/2000 (10%)]\tLoss: 1.104171\n",
      "Epoch: 5 [400/2000 (20%)]\tLoss: 1.591931\n",
      "Epoch: 5 [600/2000 (30%)]\tLoss: 1.034425\n",
      "Epoch: 5 [800/2000 (40%)]\tLoss: 0.756278\n",
      "Epoch: 5 [1000/2000 (50%)]\tLoss: 0.600105\n",
      "Epoch: 5 [1200/2000 (60%)]\tLoss: 0.818640\n",
      "Epoch: 5 [1400/2000 (70%)]\tLoss: 0.691461\n",
      "Epoch: 5 [1600/2000 (80%)]\tLoss: 0.412411\n",
      "Epoch: 5 [1800/2000 (90%)]\tLoss: 0.291399\n",
      "Epoch: 6 [0/2000 (0%)]\tLoss: 0.315557\n",
      "Epoch: 6 [200/2000 (10%)]\tLoss: 0.652136\n",
      "Epoch: 6 [400/2000 (20%)]\tLoss: 0.977968\n",
      "Epoch: 6 [600/2000 (30%)]\tLoss: 0.394185\n",
      "Epoch: 6 [800/2000 (40%)]\tLoss: 0.112471\n",
      "Epoch: 6 [1000/2000 (50%)]\tLoss: 0.991410\n",
      "Epoch: 6 [1200/2000 (60%)]\tLoss: 1.042666\n",
      "Epoch: 6 [1400/2000 (70%)]\tLoss: 0.243051\n",
      "Epoch: 6 [1600/2000 (80%)]\tLoss: 0.164108\n",
      "Epoch: 6 [1800/2000 (90%)]\tLoss: 0.132247\n",
      "Epoch: 7 [0/2000 (0%)]\tLoss: 0.139951\n",
      "Epoch: 7 [200/2000 (10%)]\tLoss: 0.257769\n",
      "Epoch: 7 [400/2000 (20%)]\tLoss: 1.017419\n",
      "Epoch: 7 [600/2000 (30%)]\tLoss: 0.838211\n",
      "Epoch: 7 [800/2000 (40%)]\tLoss: 0.513891\n",
      "Epoch: 7 [1000/2000 (50%)]\tLoss: 0.386862\n",
      "Epoch: 7 [1200/2000 (60%)]\tLoss: 0.602721\n",
      "Epoch: 7 [1400/2000 (70%)]\tLoss: 0.289122\n",
      "Epoch: 7 [1600/2000 (80%)]\tLoss: 0.091964\n",
      "Epoch: 7 [1800/2000 (90%)]\tLoss: 0.374360\n",
      "Epoch: 8 [0/2000 (0%)]\tLoss: 0.017462\n",
      "Epoch: 8 [200/2000 (10%)]\tLoss: 0.017698\n",
      "Epoch: 8 [400/2000 (20%)]\tLoss: 0.018238\n",
      "Epoch: 8 [600/2000 (30%)]\tLoss: 0.052985\n",
      "Epoch: 8 [800/2000 (40%)]\tLoss: 0.244007\n",
      "Epoch: 8 [1000/2000 (50%)]\tLoss: 0.203022\n",
      "Epoch: 8 [1200/2000 (60%)]\tLoss: 0.024118\n",
      "Epoch: 8 [1400/2000 (70%)]\tLoss: 0.056517\n",
      "Epoch: 8 [1600/2000 (80%)]\tLoss: 0.175966\n",
      "Epoch: 8 [1800/2000 (90%)]\tLoss: 0.380318\n",
      "Epoch: 9 [0/2000 (0%)]\tLoss: 0.032621\n",
      "Epoch: 9 [200/2000 (10%)]\tLoss: 0.080370\n",
      "Epoch: 9 [400/2000 (20%)]\tLoss: 0.000969\n",
      "Epoch: 9 [600/2000 (30%)]\tLoss: 0.015068\n",
      "Epoch: 9 [800/2000 (40%)]\tLoss: 0.002283\n",
      "Epoch: 9 [1000/2000 (50%)]\tLoss: 0.518229\n",
      "Epoch: 9 [1200/2000 (60%)]\tLoss: 0.060112\n",
      "Epoch: 9 [1400/2000 (70%)]\tLoss: 0.035363\n",
      "Epoch: 9 [1600/2000 (80%)]\tLoss: 0.005274\n",
      "Epoch: 9 [1800/2000 (90%)]\tLoss: 0.000063\n",
      "Epoch: 10 [0/2000 (0%)]\tLoss: 0.015421\n",
      "Epoch: 10 [200/2000 (10%)]\tLoss: 0.051841\n",
      "Epoch: 10 [400/2000 (20%)]\tLoss: 0.001964\n",
      "Epoch: 10 [600/2000 (30%)]\tLoss: 0.060519\n",
      "Epoch: 10 [800/2000 (40%)]\tLoss: 0.219858\n",
      "Epoch: 10 [1000/2000 (50%)]\tLoss: 0.048913\n",
      "Epoch: 10 [1200/2000 (60%)]\tLoss: 0.512034\n",
      "Epoch: 10 [1400/2000 (70%)]\tLoss: 0.139141\n",
      "Epoch: 10 [1600/2000 (80%)]\tLoss: 0.010562\n",
      "Epoch: 10 [1800/2000 (90%)]\tLoss: 0.003632\n",
      "Epoch: 11 [0/2000 (0%)]\tLoss: 0.014818\n",
      "Epoch: 11 [200/2000 (10%)]\tLoss: 1.307398\n",
      "Epoch: 11 [400/2000 (20%)]\tLoss: 0.023694\n",
      "Epoch: 11 [600/2000 (30%)]\tLoss: 0.811179\n",
      "Epoch: 11 [800/2000 (40%)]\tLoss: 0.036432\n",
      "Epoch: 11 [1000/2000 (50%)]\tLoss: 0.090416\n",
      "Epoch: 11 [1200/2000 (60%)]\tLoss: 0.526801\n",
      "Epoch: 11 [1400/2000 (70%)]\tLoss: 0.005834\n",
      "Epoch: 11 [1600/2000 (80%)]\tLoss: 0.215050\n",
      "Epoch: 11 [1800/2000 (90%)]\tLoss: 0.024934\n",
      "Epoch: 12 [0/2000 (0%)]\tLoss: 0.291814\n",
      "Epoch: 12 [200/2000 (10%)]\tLoss: 0.069854\n",
      "Epoch: 12 [400/2000 (20%)]\tLoss: 0.000984\n",
      "Epoch: 12 [600/2000 (30%)]\tLoss: 0.003226\n",
      "Epoch: 12 [800/2000 (40%)]\tLoss: 0.005931\n",
      "Epoch: 12 [1000/2000 (50%)]\tLoss: 0.017743\n",
      "Epoch: 12 [1200/2000 (60%)]\tLoss: 0.011824\n",
      "Epoch: 12 [1400/2000 (70%)]\tLoss: 0.006248\n",
      "Epoch: 12 [1600/2000 (80%)]\tLoss: 0.085797\n",
      "Epoch: 12 [1800/2000 (90%)]\tLoss: 0.004322\n",
      "Epoch: 13 [0/2000 (0%)]\tLoss: 0.023660\n",
      "Epoch: 13 [200/2000 (10%)]\tLoss: 0.005185\n",
      "Epoch: 13 [400/2000 (20%)]\tLoss: 0.000124\n",
      "Epoch: 13 [600/2000 (30%)]\tLoss: 0.000093\n",
      "Epoch: 13 [800/2000 (40%)]\tLoss: 0.003816\n",
      "Epoch: 13 [1000/2000 (50%)]\tLoss: 0.000821\n",
      "Epoch: 13 [1200/2000 (60%)]\tLoss: 0.000375\n",
      "Epoch: 13 [1400/2000 (70%)]\tLoss: 0.036559\n",
      "Epoch: 13 [1600/2000 (80%)]\tLoss: 0.031408\n",
      "Epoch: 13 [1800/2000 (90%)]\tLoss: 0.013758\n",
      "Epoch: 14 [0/2000 (0%)]\tLoss: 0.013969\n",
      "Epoch: 14 [200/2000 (10%)]\tLoss: 1.222420\n",
      "Epoch: 14 [400/2000 (20%)]\tLoss: 0.040945\n",
      "Epoch: 14 [600/2000 (30%)]\tLoss: 0.001292\n",
      "Epoch: 14 [800/2000 (40%)]\tLoss: 0.001523\n",
      "Epoch: 14 [1000/2000 (50%)]\tLoss: 0.002101\n",
      "Epoch: 14 [1200/2000 (60%)]\tLoss: 0.001030\n",
      "Epoch: 14 [1400/2000 (70%)]\tLoss: 0.339679\n",
      "Epoch: 14 [1600/2000 (80%)]\tLoss: 0.268007\n",
      "Epoch: 14 [1800/2000 (90%)]\tLoss: 0.001229\n",
      "Epoch: 15 [0/2000 (0%)]\tLoss: 0.001655\n",
      "Epoch: 15 [200/2000 (10%)]\tLoss: 0.053275\n",
      "Epoch: 15 [400/2000 (20%)]\tLoss: 0.054459\n",
      "Epoch: 15 [600/2000 (30%)]\tLoss: 0.009683\n",
      "Epoch: 15 [800/2000 (40%)]\tLoss: 0.012212\n",
      "Epoch: 15 [1000/2000 (50%)]\tLoss: 0.004152\n",
      "Epoch: 15 [1200/2000 (60%)]\tLoss: 0.000251\n",
      "Epoch: 15 [1400/2000 (70%)]\tLoss: 0.004807\n",
      "Epoch: 15 [1600/2000 (80%)]\tLoss: 0.032189\n",
      "Epoch: 15 [1800/2000 (90%)]\tLoss: 0.000533\n",
      "Epoch: 16 [0/2000 (0%)]\tLoss: 0.011282\n",
      "Epoch: 16 [200/2000 (10%)]\tLoss: 0.009375\n",
      "Epoch: 16 [400/2000 (20%)]\tLoss: 0.001182\n",
      "Epoch: 16 [600/2000 (30%)]\tLoss: 0.003795\n",
      "Epoch: 16 [800/2000 (40%)]\tLoss: 0.000106\n",
      "Epoch: 16 [1000/2000 (50%)]\tLoss: 1.045244\n",
      "Epoch: 16 [1200/2000 (60%)]\tLoss: 0.000769\n",
      "Epoch: 16 [1400/2000 (70%)]\tLoss: 0.006376\n",
      "Epoch: 16 [1600/2000 (80%)]\tLoss: 0.000508\n",
      "Epoch: 16 [1800/2000 (90%)]\tLoss: 0.000391\n",
      "Epoch: 17 [0/2000 (0%)]\tLoss: 0.003475\n",
      "Epoch: 17 [200/2000 (10%)]\tLoss: 0.003668\n",
      "Epoch: 17 [400/2000 (20%)]\tLoss: 0.000541\n",
      "Epoch: 17 [600/2000 (30%)]\tLoss: 0.000176\n",
      "Epoch: 17 [800/2000 (40%)]\tLoss: 0.004733\n",
      "Epoch: 17 [1000/2000 (50%)]\tLoss: 0.003774\n",
      "Epoch: 17 [1200/2000 (60%)]\tLoss: 0.011229\n",
      "Epoch: 17 [1400/2000 (70%)]\tLoss: 0.000560\n",
      "Epoch: 17 [1600/2000 (80%)]\tLoss: 0.000177\n",
      "Epoch: 17 [1800/2000 (90%)]\tLoss: 0.004371\n",
      "Epoch: 18 [0/2000 (0%)]\tLoss: 0.353349\n",
      "Epoch: 18 [200/2000 (10%)]\tLoss: 0.000923\n",
      "Epoch: 18 [400/2000 (20%)]\tLoss: 0.000096\n",
      "Epoch: 18 [600/2000 (30%)]\tLoss: 0.000750\n",
      "Epoch: 18 [800/2000 (40%)]\tLoss: 0.000312\n",
      "Epoch: 18 [1000/2000 (50%)]\tLoss: 0.000161\n",
      "Epoch: 18 [1200/2000 (60%)]\tLoss: 0.021412\n",
      "Epoch: 18 [1400/2000 (70%)]\tLoss: 0.000195\n",
      "Epoch: 18 [1600/2000 (80%)]\tLoss: 0.000096\n",
      "Epoch: 18 [1800/2000 (90%)]\tLoss: 0.000712\n",
      "Epoch: 19 [0/2000 (0%)]\tLoss: 0.266632\n",
      "Epoch: 19 [200/2000 (10%)]\tLoss: 0.000161\n",
      "Epoch: 19 [400/2000 (20%)]\tLoss: 0.002308\n",
      "Epoch: 19 [600/2000 (30%)]\tLoss: 0.005329\n",
      "Epoch: 19 [800/2000 (40%)]\tLoss: 0.004461\n",
      "Epoch: 19 [1000/2000 (50%)]\tLoss: 0.001952\n",
      "Epoch: 19 [1200/2000 (60%)]\tLoss: 0.000322\n",
      "Epoch: 19 [1400/2000 (70%)]\tLoss: 0.000142\n",
      "Epoch: 19 [1600/2000 (80%)]\tLoss: 0.024585\n",
      "Epoch: 19 [1800/2000 (90%)]\tLoss: 0.000062\n",
      "Epoch: 20 [0/2000 (0%)]\tLoss: 0.000127\n",
      "Epoch: 20 [200/2000 (10%)]\tLoss: 0.000344\n",
      "Epoch: 20 [400/2000 (20%)]\tLoss: 0.000990\n",
      "Epoch: 20 [600/2000 (30%)]\tLoss: 0.001066\n",
      "Epoch: 20 [800/2000 (40%)]\tLoss: 0.000203\n",
      "Epoch: 20 [1000/2000 (50%)]\tLoss: 0.001898\n",
      "Epoch: 20 [1200/2000 (60%)]\tLoss: 0.000098\n",
      "Epoch: 20 [1400/2000 (70%)]\tLoss: 0.000368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 [1600/2000 (80%)]\tLoss: 0.002066\n",
      "Epoch: 20 [1800/2000 (90%)]\tLoss: 0.001212\n",
      "Epoch: 21 [0/2000 (0%)]\tLoss: 0.000197\n",
      "Epoch: 21 [200/2000 (10%)]\tLoss: 0.000000\n",
      "Epoch: 21 [400/2000 (20%)]\tLoss: 0.000043\n",
      "Epoch: 21 [600/2000 (30%)]\tLoss: 0.000836\n",
      "Epoch: 21 [800/2000 (40%)]\tLoss: 0.003576\n",
      "Epoch: 21 [1000/2000 (50%)]\tLoss: 0.018436\n",
      "Epoch: 21 [1200/2000 (60%)]\tLoss: 0.010950\n",
      "Epoch: 21 [1400/2000 (70%)]\tLoss: 0.000486\n",
      "Epoch: 21 [1600/2000 (80%)]\tLoss: 0.009331\n",
      "Epoch: 21 [1800/2000 (90%)]\tLoss: 0.001520\n",
      "Epoch: 22 [0/2000 (0%)]\tLoss: 0.018935\n",
      "Epoch: 22 [200/2000 (10%)]\tLoss: 0.000406\n",
      "Epoch: 22 [400/2000 (20%)]\tLoss: 0.000100\n",
      "Epoch: 22 [600/2000 (30%)]\tLoss: 0.000080\n",
      "Epoch: 22 [800/2000 (40%)]\tLoss: 0.000190\n",
      "Epoch: 22 [1000/2000 (50%)]\tLoss: 0.585946\n",
      "Epoch: 22 [1200/2000 (60%)]\tLoss: 0.082623\n",
      "Epoch: 22 [1400/2000 (70%)]\tLoss: 0.111226\n",
      "Epoch: 22 [1600/2000 (80%)]\tLoss: 0.008869\n",
      "Epoch: 22 [1800/2000 (90%)]\tLoss: 0.019797\n",
      "Epoch: 23 [0/2000 (0%)]\tLoss: 0.173087\n",
      "Epoch: 23 [200/2000 (10%)]\tLoss: 0.000394\n",
      "Epoch: 23 [400/2000 (20%)]\tLoss: 0.000873\n",
      "Epoch: 23 [600/2000 (30%)]\tLoss: 0.000053\n",
      "Epoch: 23 [800/2000 (40%)]\tLoss: 0.000029\n",
      "Epoch: 23 [1000/2000 (50%)]\tLoss: 0.000312\n",
      "Epoch: 23 [1200/2000 (60%)]\tLoss: 0.001362\n",
      "Epoch: 23 [1400/2000 (70%)]\tLoss: 0.008363\n",
      "Epoch: 23 [1600/2000 (80%)]\tLoss: 0.000063\n",
      "Epoch: 23 [1800/2000 (90%)]\tLoss: 0.000555\n",
      "Epoch: 24 [0/2000 (0%)]\tLoss: 0.000517\n",
      "Epoch: 24 [200/2000 (10%)]\tLoss: 0.000277\n",
      "Epoch: 24 [400/2000 (20%)]\tLoss: 0.000247\n",
      "Epoch: 24 [600/2000 (30%)]\tLoss: 0.000377\n",
      "Epoch: 24 [800/2000 (40%)]\tLoss: 0.000711\n",
      "Epoch: 24 [1000/2000 (50%)]\tLoss: 0.002063\n",
      "Epoch: 24 [1200/2000 (60%)]\tLoss: 0.001589\n",
      "Epoch: 24 [1400/2000 (70%)]\tLoss: 0.000109\n",
      "Epoch: 24 [1600/2000 (80%)]\tLoss: 0.007556\n",
      "Epoch: 24 [1800/2000 (90%)]\tLoss: 0.000032\n",
      "Epoch: 25 [0/2000 (0%)]\tLoss: 0.001910\n",
      "Epoch: 25 [200/2000 (10%)]\tLoss: 0.000035\n",
      "Epoch: 25 [400/2000 (20%)]\tLoss: 0.001529\n",
      "Epoch: 25 [600/2000 (30%)]\tLoss: 0.000012\n",
      "Epoch: 25 [800/2000 (40%)]\tLoss: 0.000175\n",
      "Epoch: 25 [1000/2000 (50%)]\tLoss: 0.000331\n",
      "Epoch: 25 [1200/2000 (60%)]\tLoss: 0.000127\n",
      "Epoch: 25 [1400/2000 (70%)]\tLoss: 0.000437\n",
      "Epoch: 25 [1600/2000 (80%)]\tLoss: 0.002512\n",
      "Epoch: 25 [1800/2000 (90%)]\tLoss: 0.003557\n",
      "Epoch: 26 [0/2000 (0%)]\tLoss: 0.000031\n",
      "Epoch: 26 [200/2000 (10%)]\tLoss: 0.000380\n",
      "Epoch: 26 [400/2000 (20%)]\tLoss: 0.000073\n",
      "Epoch: 26 [600/2000 (30%)]\tLoss: 0.000825\n",
      "Epoch: 26 [800/2000 (40%)]\tLoss: 0.001360\n",
      "Epoch: 26 [1000/2000 (50%)]\tLoss: 0.000019\n",
      "Epoch: 26 [1200/2000 (60%)]\tLoss: 0.000202\n",
      "Epoch: 26 [1400/2000 (70%)]\tLoss: 0.000135\n",
      "Epoch: 26 [1600/2000 (80%)]\tLoss: 0.001371\n",
      "Epoch: 26 [1800/2000 (90%)]\tLoss: 0.000267\n",
      "Epoch: 27 [0/2000 (0%)]\tLoss: 0.000108\n",
      "Epoch: 27 [200/2000 (10%)]\tLoss: 0.000038\n",
      "Epoch: 27 [400/2000 (20%)]\tLoss: 0.002886\n",
      "Epoch: 27 [600/2000 (30%)]\tLoss: 0.000109\n",
      "Epoch: 27 [800/2000 (40%)]\tLoss: 0.001328\n",
      "Epoch: 27 [1000/2000 (50%)]\tLoss: 0.000027\n",
      "Epoch: 27 [1200/2000 (60%)]\tLoss: 0.009217\n",
      "Epoch: 27 [1400/2000 (70%)]\tLoss: 0.000052\n",
      "Epoch: 27 [1600/2000 (80%)]\tLoss: 0.000015\n",
      "Epoch: 27 [1800/2000 (90%)]\tLoss: 0.000013\n",
      "Epoch: 28 [0/2000 (0%)]\tLoss: 0.000921\n",
      "Epoch: 28 [200/2000 (10%)]\tLoss: 0.000051\n",
      "Epoch: 28 [400/2000 (20%)]\tLoss: 0.000984\n",
      "Epoch: 28 [600/2000 (30%)]\tLoss: 0.000925\n",
      "Epoch: 28 [800/2000 (40%)]\tLoss: 0.000202\n",
      "Epoch: 28 [1000/2000 (50%)]\tLoss: 0.000045\n",
      "Epoch: 28 [1200/2000 (60%)]\tLoss: 0.000134\n",
      "Epoch: 28 [1400/2000 (70%)]\tLoss: 0.000611\n",
      "Epoch: 28 [1600/2000 (80%)]\tLoss: 0.001405\n",
      "Epoch: 28 [1800/2000 (90%)]\tLoss: 0.000110\n",
      "Epoch: 29 [0/2000 (0%)]\tLoss: 0.000538\n",
      "Epoch: 29 [200/2000 (10%)]\tLoss: 0.000018\n",
      "Epoch: 29 [400/2000 (20%)]\tLoss: 0.000034\n",
      "Epoch: 29 [600/2000 (30%)]\tLoss: 0.000149\n",
      "Epoch: 29 [800/2000 (40%)]\tLoss: 0.000153\n",
      "Epoch: 29 [1000/2000 (50%)]\tLoss: 0.000153\n",
      "Epoch: 29 [1200/2000 (60%)]\tLoss: 0.000014\n",
      "Epoch: 29 [1400/2000 (70%)]\tLoss: 0.000923\n",
      "Epoch: 29 [1600/2000 (80%)]\tLoss: 0.002090\n",
      "Epoch: 29 [1800/2000 (90%)]\tLoss: 0.000010\n",
      "Epoch: 30 [0/2000 (0%)]\tLoss: 0.000000\n",
      "Epoch: 30 [200/2000 (10%)]\tLoss: 0.000100\n",
      "Epoch: 30 [400/2000 (20%)]\tLoss: 0.000019\n",
      "Epoch: 30 [600/2000 (30%)]\tLoss: 0.000018\n",
      "Epoch: 30 [800/2000 (40%)]\tLoss: 0.000060\n",
      "Epoch: 30 [1000/2000 (50%)]\tLoss: 0.000692\n",
      "Epoch: 30 [1200/2000 (60%)]\tLoss: 0.000036\n",
      "Epoch: 30 [1400/2000 (70%)]\tLoss: 0.000037\n",
      "Epoch: 30 [1600/2000 (80%)]\tLoss: 0.000177\n",
      "Epoch: 30 [1800/2000 (90%)]\tLoss: 0.000040\n",
      "Epoch: 31 [0/2000 (0%)]\tLoss: 0.000065\n",
      "Epoch: 31 [200/2000 (10%)]\tLoss: 0.000110\n",
      "Epoch: 31 [400/2000 (20%)]\tLoss: 0.001289\n",
      "Epoch: 31 [600/2000 (30%)]\tLoss: 0.000154\n",
      "Epoch: 31 [800/2000 (40%)]\tLoss: 0.000056\n",
      "Epoch: 31 [1000/2000 (50%)]\tLoss: 0.000070\n",
      "Epoch: 31 [1200/2000 (60%)]\tLoss: 0.000011\n",
      "Epoch: 31 [1400/2000 (70%)]\tLoss: 0.000982\n",
      "Epoch: 31 [1600/2000 (80%)]\tLoss: 0.000010\n",
      "Epoch: 31 [1800/2000 (90%)]\tLoss: 0.000041\n",
      "Epoch: 32 [0/2000 (0%)]\tLoss: 0.001650\n",
      "Epoch: 32 [200/2000 (10%)]\tLoss: 0.000217\n",
      "Epoch: 32 [400/2000 (20%)]\tLoss: 0.000057\n",
      "Epoch: 32 [600/2000 (30%)]\tLoss: 0.000049\n",
      "Epoch: 32 [800/2000 (40%)]\tLoss: 0.000008\n",
      "Epoch: 32 [1000/2000 (50%)]\tLoss: 0.000140\n",
      "Epoch: 32 [1200/2000 (60%)]\tLoss: 0.000162\n",
      "Epoch: 32 [1400/2000 (70%)]\tLoss: 0.000002\n",
      "Epoch: 32 [1600/2000 (80%)]\tLoss: 0.000024\n",
      "Epoch: 32 [1800/2000 (90%)]\tLoss: 0.000001\n",
      "Epoch: 33 [0/2000 (0%)]\tLoss: 0.000003\n",
      "Epoch: 33 [200/2000 (10%)]\tLoss: 0.000226\n",
      "Epoch: 33 [400/2000 (20%)]\tLoss: 0.000159\n",
      "Epoch: 33 [600/2000 (30%)]\tLoss: 0.000270\n",
      "Epoch: 33 [800/2000 (40%)]\tLoss: 0.000225\n",
      "Epoch: 33 [1000/2000 (50%)]\tLoss: 0.000136\n",
      "Epoch: 33 [1200/2000 (60%)]\tLoss: 0.000115\n",
      "Epoch: 33 [1400/2000 (70%)]\tLoss: 0.001230\n",
      "Epoch: 33 [1600/2000 (80%)]\tLoss: 0.000139\n",
      "Epoch: 33 [1800/2000 (90%)]\tLoss: 0.000276\n",
      "Epoch: 34 [0/2000 (0%)]\tLoss: 0.000086\n",
      "Epoch: 34 [200/2000 (10%)]\tLoss: 0.000017\n",
      "Epoch: 34 [400/2000 (20%)]\tLoss: 0.000007\n",
      "Epoch: 34 [600/2000 (30%)]\tLoss: 0.000065\n",
      "Epoch: 34 [800/2000 (40%)]\tLoss: 0.000000\n",
      "Epoch: 34 [1000/2000 (50%)]\tLoss: 0.000507\n",
      "Epoch: 34 [1200/2000 (60%)]\tLoss: 0.000086\n",
      "Epoch: 34 [1400/2000 (70%)]\tLoss: 0.000001\n",
      "Epoch: 34 [1600/2000 (80%)]\tLoss: 0.000002\n",
      "Epoch: 34 [1800/2000 (90%)]\tLoss: 0.000372\n",
      "Epoch: 35 [0/2000 (0%)]\tLoss: 0.000321\n",
      "Epoch: 35 [200/2000 (10%)]\tLoss: 0.000089\n",
      "Epoch: 35 [400/2000 (20%)]\tLoss: 0.000013\n",
      "Epoch: 35 [600/2000 (30%)]\tLoss: 0.000028\n",
      "Epoch: 35 [800/2000 (40%)]\tLoss: 0.000055\n",
      "Epoch: 35 [1000/2000 (50%)]\tLoss: 0.000007\n",
      "Epoch: 35 [1200/2000 (60%)]\tLoss: 0.000122\n",
      "Epoch: 35 [1400/2000 (70%)]\tLoss: 0.000870\n",
      "Epoch: 35 [1600/2000 (80%)]\tLoss: 0.000089\n",
      "Epoch: 35 [1800/2000 (90%)]\tLoss: 0.000164\n",
      "Epoch: 36 [0/2000 (0%)]\tLoss: 0.000078\n",
      "Epoch: 36 [200/2000 (10%)]\tLoss: 0.000499\n",
      "Epoch: 36 [400/2000 (20%)]\tLoss: 0.000704\n",
      "Epoch: 36 [600/2000 (30%)]\tLoss: 0.000123\n",
      "Epoch: 36 [800/2000 (40%)]\tLoss: 0.000822\n",
      "Epoch: 36 [1000/2000 (50%)]\tLoss: 0.000027\n",
      "Epoch: 36 [1200/2000 (60%)]\tLoss: 0.000003\n",
      "Epoch: 36 [1400/2000 (70%)]\tLoss: 0.000119\n",
      "Epoch: 36 [1600/2000 (80%)]\tLoss: 0.000099\n",
      "Epoch: 36 [1800/2000 (90%)]\tLoss: 0.000300\n",
      "Epoch: 37 [0/2000 (0%)]\tLoss: 0.000004\n",
      "Epoch: 37 [200/2000 (10%)]\tLoss: 0.000508\n",
      "Epoch: 37 [400/2000 (20%)]\tLoss: 0.000217\n",
      "Epoch: 37 [600/2000 (30%)]\tLoss: 0.000042\n",
      "Epoch: 37 [800/2000 (40%)]\tLoss: 0.000004\n",
      "Epoch: 37 [1000/2000 (50%)]\tLoss: 0.000016\n",
      "Epoch: 37 [1200/2000 (60%)]\tLoss: 0.000412\n",
      "Epoch: 37 [1400/2000 (70%)]\tLoss: 0.000076\n",
      "Epoch: 37 [1600/2000 (80%)]\tLoss: 0.000005\n",
      "Epoch: 37 [1800/2000 (90%)]\tLoss: 0.000021\n",
      "Epoch: 38 [0/2000 (0%)]\tLoss: 0.000054\n",
      "Epoch: 38 [200/2000 (10%)]\tLoss: 0.000296\n",
      "Epoch: 38 [400/2000 (20%)]\tLoss: 0.000341\n",
      "Epoch: 38 [600/2000 (30%)]\tLoss: 0.000139\n",
      "Epoch: 38 [800/2000 (40%)]\tLoss: 0.000037\n",
      "Epoch: 38 [1000/2000 (50%)]\tLoss: 0.000096\n",
      "Epoch: 38 [1200/2000 (60%)]\tLoss: 0.000001\n",
      "Epoch: 38 [1400/2000 (70%)]\tLoss: 0.000013\n",
      "Epoch: 38 [1600/2000 (80%)]\tLoss: 0.000314\n",
      "Epoch: 38 [1800/2000 (90%)]\tLoss: 0.000000\n",
      "Epoch: 39 [0/2000 (0%)]\tLoss: 0.000002\n",
      "Epoch: 39 [200/2000 (10%)]\tLoss: 0.000000\n",
      "Epoch: 39 [400/2000 (20%)]\tLoss: 0.000008\n",
      "Epoch: 39 [600/2000 (30%)]\tLoss: 0.000043\n",
      "Epoch: 39 [800/2000 (40%)]\tLoss: 0.000057\n",
      "Epoch: 39 [1000/2000 (50%)]\tLoss: 0.000090\n",
      "Epoch: 39 [1200/2000 (60%)]\tLoss: 0.000173\n",
      "Epoch: 39 [1400/2000 (70%)]\tLoss: 0.000044\n",
      "Epoch: 39 [1600/2000 (80%)]\tLoss: 0.000070\n",
      "Epoch: 39 [1800/2000 (90%)]\tLoss: 0.000003\n",
      "Epoch: 40 [0/2000 (0%)]\tLoss: 0.000039\n",
      "Epoch: 40 [200/2000 (10%)]\tLoss: 0.000021\n",
      "Epoch: 40 [400/2000 (20%)]\tLoss: 0.000071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 [600/2000 (30%)]\tLoss: 0.000832\n",
      "Epoch: 40 [800/2000 (40%)]\tLoss: 0.000012\n",
      "Epoch: 40 [1000/2000 (50%)]\tLoss: 0.000018\n",
      "Epoch: 40 [1200/2000 (60%)]\tLoss: 0.000060\n",
      "Epoch: 40 [1400/2000 (70%)]\tLoss: 0.000066\n",
      "Epoch: 40 [1600/2000 (80%)]\tLoss: 0.000016\n",
      "Epoch: 40 [1800/2000 (90%)]\tLoss: 0.000352\n",
      "Epoch: 41 [0/2000 (0%)]\tLoss: 0.000067\n",
      "Epoch: 41 [200/2000 (10%)]\tLoss: 0.000197\n",
      "Epoch: 41 [400/2000 (20%)]\tLoss: 0.000128\n",
      "Epoch: 41 [600/2000 (30%)]\tLoss: 0.000117\n",
      "Epoch: 41 [800/2000 (40%)]\tLoss: 0.000002\n",
      "Epoch: 41 [1000/2000 (50%)]\tLoss: 0.000250\n",
      "Epoch: 41 [1200/2000 (60%)]\tLoss: 0.000145\n",
      "Epoch: 41 [1400/2000 (70%)]\tLoss: 0.000043\n",
      "Epoch: 41 [1600/2000 (80%)]\tLoss: 0.000009\n",
      "Epoch: 41 [1800/2000 (90%)]\tLoss: 0.000028\n",
      "Epoch: 42 [0/2000 (0%)]\tLoss: 0.000009\n",
      "Epoch: 42 [200/2000 (10%)]\tLoss: 0.000346\n",
      "Epoch: 42 [400/2000 (20%)]\tLoss: 0.000159\n",
      "Epoch: 42 [600/2000 (30%)]\tLoss: 0.000037\n",
      "Epoch: 42 [800/2000 (40%)]\tLoss: 0.000161\n",
      "Epoch: 42 [1000/2000 (50%)]\tLoss: 0.000081\n",
      "Epoch: 42 [1200/2000 (60%)]\tLoss: 0.000088\n",
      "Epoch: 42 [1400/2000 (70%)]\tLoss: 0.000000\n",
      "Epoch: 42 [1600/2000 (80%)]\tLoss: 0.000359\n",
      "Epoch: 42 [1800/2000 (90%)]\tLoss: 0.000391\n",
      "Epoch: 43 [0/2000 (0%)]\tLoss: 0.000043\n",
      "Epoch: 43 [200/2000 (10%)]\tLoss: 0.000023\n",
      "Epoch: 43 [400/2000 (20%)]\tLoss: 0.000016\n",
      "Epoch: 43 [600/2000 (30%)]\tLoss: 0.000047\n",
      "Epoch: 43 [800/2000 (40%)]\tLoss: 0.000012\n",
      "Epoch: 43 [1000/2000 (50%)]\tLoss: 0.000221\n",
      "Epoch: 43 [1200/2000 (60%)]\tLoss: 0.000302\n",
      "Epoch: 43 [1400/2000 (70%)]\tLoss: 0.000036\n",
      "Epoch: 43 [1600/2000 (80%)]\tLoss: 0.000007\n",
      "Epoch: 43 [1800/2000 (90%)]\tLoss: 0.000037\n",
      "Epoch: 44 [0/2000 (0%)]\tLoss: 0.000046\n",
      "Epoch: 44 [200/2000 (10%)]\tLoss: 0.000034\n",
      "Epoch: 44 [400/2000 (20%)]\tLoss: 0.000000\n",
      "Epoch: 44 [600/2000 (30%)]\tLoss: 0.000023\n",
      "Epoch: 44 [800/2000 (40%)]\tLoss: 0.000070\n",
      "Epoch: 44 [1000/2000 (50%)]\tLoss: 0.000397\n",
      "Epoch: 44 [1200/2000 (60%)]\tLoss: 0.000106\n",
      "Epoch: 44 [1400/2000 (70%)]\tLoss: 0.000113\n",
      "Epoch: 44 [1600/2000 (80%)]\tLoss: 0.000025\n",
      "Epoch: 44 [1800/2000 (90%)]\tLoss: 0.000017\n",
      "Epoch: 45 [0/2000 (0%)]\tLoss: 0.000052\n",
      "Epoch: 45 [200/2000 (10%)]\tLoss: 0.000005\n",
      "Epoch: 45 [400/2000 (20%)]\tLoss: 0.000004\n",
      "Epoch: 45 [600/2000 (30%)]\tLoss: 0.000048\n",
      "Epoch: 45 [800/2000 (40%)]\tLoss: 0.000034\n",
      "Epoch: 45 [1000/2000 (50%)]\tLoss: 0.000072\n",
      "Epoch: 45 [1200/2000 (60%)]\tLoss: 0.000410\n",
      "Epoch: 45 [1400/2000 (70%)]\tLoss: 0.000057\n",
      "Epoch: 45 [1600/2000 (80%)]\tLoss: 0.000082\n",
      "Epoch: 45 [1800/2000 (90%)]\tLoss: 0.000043\n",
      "Epoch: 46 [0/2000 (0%)]\tLoss: 0.000052\n",
      "Epoch: 46 [200/2000 (10%)]\tLoss: 0.000003\n",
      "Epoch: 46 [400/2000 (20%)]\tLoss: 0.000102\n",
      "Epoch: 46 [600/2000 (30%)]\tLoss: 0.000011\n",
      "Epoch: 46 [800/2000 (40%)]\tLoss: 0.000234\n",
      "Epoch: 46 [1000/2000 (50%)]\tLoss: 0.000034\n",
      "Epoch: 46 [1200/2000 (60%)]\tLoss: 0.000040\n",
      "Epoch: 46 [1400/2000 (70%)]\tLoss: 0.000050\n",
      "Epoch: 46 [1600/2000 (80%)]\tLoss: 0.000521\n",
      "Epoch: 46 [1800/2000 (90%)]\tLoss: 0.000024\n",
      "Epoch: 47 [0/2000 (0%)]\tLoss: 0.000054\n",
      "Epoch: 47 [200/2000 (10%)]\tLoss: 0.000006\n",
      "Epoch: 47 [400/2000 (20%)]\tLoss: 0.000010\n",
      "Epoch: 47 [600/2000 (30%)]\tLoss: 0.000032\n",
      "Epoch: 47 [800/2000 (40%)]\tLoss: 0.000036\n",
      "Epoch: 47 [1000/2000 (50%)]\tLoss: 0.000002\n",
      "Epoch: 47 [1200/2000 (60%)]\tLoss: 0.000048\n",
      "Epoch: 47 [1400/2000 (70%)]\tLoss: 0.000001\n",
      "Epoch: 47 [1600/2000 (80%)]\tLoss: 0.000010\n",
      "Epoch: 47 [1800/2000 (90%)]\tLoss: 0.000000\n",
      "Epoch: 48 [0/2000 (0%)]\tLoss: 0.000154\n",
      "Epoch: 48 [200/2000 (10%)]\tLoss: 0.000027\n",
      "Epoch: 48 [400/2000 (20%)]\tLoss: 0.000329\n",
      "Epoch: 48 [600/2000 (30%)]\tLoss: 0.000078\n",
      "Epoch: 48 [800/2000 (40%)]\tLoss: 0.000080\n",
      "Epoch: 48 [1000/2000 (50%)]\tLoss: 0.000066\n",
      "Epoch: 48 [1200/2000 (60%)]\tLoss: 0.000126\n",
      "Epoch: 48 [1400/2000 (70%)]\tLoss: 0.000008\n",
      "Epoch: 48 [1600/2000 (80%)]\tLoss: 0.000001\n",
      "Epoch: 48 [1800/2000 (90%)]\tLoss: 0.000032\n",
      "Epoch: 49 [0/2000 (0%)]\tLoss: 0.000061\n",
      "Epoch: 49 [200/2000 (10%)]\tLoss: 0.000003\n",
      "Epoch: 49 [400/2000 (20%)]\tLoss: 0.000108\n",
      "Epoch: 49 [600/2000 (30%)]\tLoss: 0.000037\n",
      "Epoch: 49 [800/2000 (40%)]\tLoss: 0.000017\n",
      "Epoch: 49 [1000/2000 (50%)]\tLoss: 0.000000\n",
      "Epoch: 49 [1200/2000 (60%)]\tLoss: 0.000069\n",
      "Epoch: 49 [1400/2000 (70%)]\tLoss: 0.000154\n",
      "Epoch: 49 [1600/2000 (80%)]\tLoss: 0.000454\n",
      "Epoch: 49 [1800/2000 (90%)]\tLoss: 0.000211\n",
      "Epoch: 50 [0/2000 (0%)]\tLoss: 0.000014\n",
      "Epoch: 50 [200/2000 (10%)]\tLoss: 0.000091\n",
      "Epoch: 50 [400/2000 (20%)]\tLoss: 0.000046\n",
      "Epoch: 50 [600/2000 (30%)]\tLoss: 0.000011\n",
      "Epoch: 50 [800/2000 (40%)]\tLoss: 0.000008\n",
      "Epoch: 50 [1000/2000 (50%)]\tLoss: 0.000334\n",
      "Epoch: 50 [1200/2000 (60%)]\tLoss: 0.000291\n",
      "Epoch: 50 [1400/2000 (70%)]\tLoss: 0.000000\n",
      "Epoch: 50 [1600/2000 (80%)]\tLoss: 0.000008\n",
      "Epoch: 50 [1800/2000 (90%)]\tLoss: 0.000036\n",
      "Finished training in  487.037 seconds \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "print(\"Started training\")\n",
    "\n",
    "epochs = 50\n",
    "print_interval = 50 #prints every p_i*4\n",
    "tempo = []\n",
    "acc = []\n",
    "\n",
    "for epoch in range(epochs):  # nbr epochs\n",
    "    for batch_idx, (data, target) in enumerate(train_loader): #nbr batch,in,out\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        #On resize pour la sortie\n",
    "        data = data.view(-1, 32*32)\n",
    "\n",
    "        #init l'entrainement\n",
    "        optimizer.zero_grad()\n",
    "        net_out = model(data)\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #afficher la progression\n",
    "        if batch_idx % print_interval == 0:\n",
    "            #le print statement le plus illisible du monde\n",
    "            print('Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "    tempo.append(epoch)\n",
    "    acc.append(loss.data[0])\n",
    "    \n",
    "print(\"Finished training in  %.3f seconds \" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et maintenant on teste :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0200, Accuracy: 96/100 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    \n",
    "    #rescale\n",
    "    data = data.view(-1, 32 * 32)\n",
    "    net_out = model(data)\n",
    "    \n",
    "    #somme des pertes du batch\n",
    "    test_loss += criterion(net_out, target).data[0]\n",
    "    pred = net_out.data.max(1)[1] #prediction\n",
    "    correct += pred.eq(target.data).sum() #output du réseau\n",
    "\n",
    "test_loss /= len(test_loader.dataset) #loss = loss/length set\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sauvegarde le modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"pytorchMCV2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7e4421f358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#joli plot du loss en fonction de l'epoch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(tempo, acc)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Variation of loss during training')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
