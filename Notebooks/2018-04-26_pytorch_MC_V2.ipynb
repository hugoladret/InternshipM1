{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018-04-26 - Améliorer le réseau\n",
    "On reprend le notebook de la veille et on améliore les performances, avec notamment une séparation correcte des batchs de training et de test. Le réseau convolutionné ne convergeait pas, donc on le rétrécit en un réseau de 3 couches linéaires.\n",
    "\n",
    "On importe les données :\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "data_transform = transforms.Compose(\n",
    "    [transforms.Grayscale(),\n",
    "     transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5,0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "train_set = datasets.ImageFolder(root='clouds_easy',\n",
    "                                transform=data_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                             batch_size=4, shuffle=True,\n",
    "                                             num_workers=1)\n",
    "\n",
    "test_set = datasets.ImageFolder(root='clouds_easy_test',\n",
    "                                transform=data_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                             batch_size=4,shuffle=False,\n",
    "                                             num_workers=1)\n",
    "#les 4 thetas qu'on essaie d'apprendre\n",
    "cloud_classes = ('0', 'pi/4', 'pi/2', '3pi/4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche des images du set importé pour vérifier que tout a bien marché :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# pour montrer une image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5    #de-normaliser\n",
    "    npimg = img.numpy()    #convertir en array\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# on loop sur un batch\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('          '.join('%s' % cloud_classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et maintenant on défini le réseau :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(32 * 32, 200)\n",
    "            self.fc2 = nn.Linear(200, 200)\n",
    "            self.fc3 = nn.Linear(200, 4)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return F.log_softmax(input=x)\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et l'optimiseur, toujours en SGD mais avec un learning rate 10 fois plus grand. Avec NLLL comme critère, on a rajouté une couche de softmax en sortie pour obtenir des log-proba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraine :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "print(\"Started training\")\n",
    "\n",
    "epochs = 50\n",
    "print_interval = 50 #prints every p_i*4\n",
    "tempo = []\n",
    "acc = []\n",
    "\n",
    "for epoch in range(epochs):  # nbr epochs\n",
    "    for batch_idx, (data, target) in enumerate(train_loader): #nbr batch,in,out\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        #On resize pour la sortie\n",
    "        data = data.view(-1, 32*32)\n",
    "\n",
    "        #init l'entrainement\n",
    "        optimizer.zero_grad()\n",
    "        net_out = model(data)\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #afficher la progression\n",
    "        if batch_idx % print_interval == 0:\n",
    "            #le print statement le plus illisible du monde\n",
    "            print('Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "    tempo.append(epoch)\n",
    "    acc.append(loss.data[0])\n",
    "    \n",
    "print(\"Finished training in  %.3f seconds \" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et maintenant on teste :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    \n",
    "    #rescale\n",
    "    data = data.view(-1, 32 * 32)\n",
    "    net_out = model(data)\n",
    "    \n",
    "    #somme des pertes du batch\n",
    "    test_loss += criterion(net_out, target).data[0]\n",
    "    pred = net_out.data.max(1)[1] #prediction\n",
    "    correct += pred.eq(target.data).sum() #output du réseau\n",
    "\n",
    "test_loss /= len(test_loader.dataset) #loss = loss/length set\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sauvegarde le modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"MODEL_trainEASY_pytorchMCV2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joli plot du loss en fonction de l'epoch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(tempo, acc)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss variation - Easy training')\n",
    "plt.savefig('Loss_easytraining.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant pour le set moyen :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [0/2000 (0%)]\tLoss: 1.405973\n",
      "Epoch: 1 [200/2000 (10%)]\tLoss: 1.472624\n",
      "Epoch: 1 [400/2000 (20%)]\tLoss: 1.472327\n",
      "Epoch: 1 [600/2000 (30%)]\tLoss: 1.401734\n",
      "Epoch: 1 [800/2000 (40%)]\tLoss: 1.275055\n",
      "Epoch: 1 [1000/2000 (50%)]\tLoss: 1.386950\n",
      "Epoch: 1 [1200/2000 (60%)]\tLoss: 1.367775\n",
      "Epoch: 1 [1400/2000 (70%)]\tLoss: 1.396947\n",
      "Epoch: 1 [1600/2000 (80%)]\tLoss: 1.370371\n",
      "Epoch: 1 [1800/2000 (90%)]\tLoss: 1.386262\n",
      "Epoch: 2 [0/2000 (0%)]\tLoss: 1.378950\n",
      "Epoch: 2 [200/2000 (10%)]\tLoss: 1.495698\n",
      "Epoch: 2 [400/2000 (20%)]\tLoss: 1.406986\n",
      "Epoch: 2 [600/2000 (30%)]\tLoss: 1.334477\n",
      "Epoch: 2 [800/2000 (40%)]\tLoss: 1.332101\n",
      "Epoch: 2 [1000/2000 (50%)]\tLoss: 1.336308\n",
      "Epoch: 2 [1200/2000 (60%)]\tLoss: 1.444397\n",
      "Epoch: 2 [1400/2000 (70%)]\tLoss: 1.479613\n",
      "Epoch: 2 [1600/2000 (80%)]\tLoss: 1.387455\n",
      "Epoch: 2 [1800/2000 (90%)]\tLoss: 1.377701\n",
      "Epoch: 3 [0/2000 (0%)]\tLoss: 1.344436\n",
      "Epoch: 3 [200/2000 (10%)]\tLoss: 1.402278\n",
      "Epoch: 3 [400/2000 (20%)]\tLoss: 1.485408\n",
      "Epoch: 3 [600/2000 (30%)]\tLoss: 1.194599\n",
      "Epoch: 3 [800/2000 (40%)]\tLoss: 1.383118\n",
      "Epoch: 3 [1000/2000 (50%)]\tLoss: 1.448466\n",
      "Epoch: 3 [1200/2000 (60%)]\tLoss: 1.343781\n",
      "Epoch: 3 [1400/2000 (70%)]\tLoss: 1.421985\n",
      "Epoch: 3 [1600/2000 (80%)]\tLoss: 1.367090\n",
      "Epoch: 3 [1800/2000 (90%)]\tLoss: 1.425695\n",
      "Epoch: 4 [0/2000 (0%)]\tLoss: 1.457182\n",
      "Epoch: 4 [200/2000 (10%)]\tLoss: 1.446516\n",
      "Epoch: 4 [400/2000 (20%)]\tLoss: 1.394779\n",
      "Epoch: 4 [600/2000 (30%)]\tLoss: 1.446670\n",
      "Epoch: 4 [800/2000 (40%)]\tLoss: 1.393290\n",
      "Epoch: 4 [1000/2000 (50%)]\tLoss: 1.382702\n",
      "Epoch: 4 [1200/2000 (60%)]\tLoss: 1.502931\n",
      "Epoch: 4 [1400/2000 (70%)]\tLoss: 1.430012\n",
      "Epoch: 4 [1600/2000 (80%)]\tLoss: 1.448594\n",
      "Epoch: 4 [1800/2000 (90%)]\tLoss: 1.382366\n",
      "Epoch: 5 [0/2000 (0%)]\tLoss: 1.419280\n",
      "Epoch: 5 [200/2000 (10%)]\tLoss: 1.303281\n",
      "Epoch: 5 [400/2000 (20%)]\tLoss: 1.369059\n",
      "Epoch: 5 [600/2000 (30%)]\tLoss: 1.447874\n",
      "Epoch: 5 [800/2000 (40%)]\tLoss: 1.291334\n",
      "Epoch: 5 [1000/2000 (50%)]\tLoss: 1.357968\n",
      "Epoch: 5 [1200/2000 (60%)]\tLoss: 1.384833\n",
      "Epoch: 5 [1400/2000 (70%)]\tLoss: 1.369420\n",
      "Epoch: 5 [1600/2000 (80%)]\tLoss: 1.393706\n",
      "Epoch: 5 [1800/2000 (90%)]\tLoss: 1.421760\n",
      "Epoch: 6 [0/2000 (0%)]\tLoss: 1.257116\n",
      "Epoch: 6 [200/2000 (10%)]\tLoss: 1.380149\n",
      "Epoch: 6 [400/2000 (20%)]\tLoss: 1.399125\n",
      "Epoch: 6 [600/2000 (30%)]\tLoss: 1.447573\n",
      "Epoch: 6 [800/2000 (40%)]\tLoss: 1.348472\n",
      "Epoch: 6 [1000/2000 (50%)]\tLoss: 1.362385\n",
      "Epoch: 6 [1200/2000 (60%)]\tLoss: 1.411478\n",
      "Epoch: 6 [1400/2000 (70%)]\tLoss: 1.398875\n",
      "Epoch: 6 [1600/2000 (80%)]\tLoss: 1.419251\n",
      "Epoch: 6 [1800/2000 (90%)]\tLoss: 1.337224\n",
      "Epoch: 7 [0/2000 (0%)]\tLoss: 1.411334\n",
      "Epoch: 7 [200/2000 (10%)]\tLoss: 1.379250\n",
      "Epoch: 7 [400/2000 (20%)]\tLoss: 1.415096\n",
      "Epoch: 7 [600/2000 (30%)]\tLoss: 1.659139\n",
      "Epoch: 7 [800/2000 (40%)]\tLoss: 1.311364\n",
      "Epoch: 7 [1000/2000 (50%)]\tLoss: 1.509413\n",
      "Epoch: 7 [1200/2000 (60%)]\tLoss: 1.242625\n",
      "Epoch: 7 [1400/2000 (70%)]\tLoss: 1.661093\n",
      "Epoch: 7 [1600/2000 (80%)]\tLoss: 1.401405\n",
      "Epoch: 7 [1800/2000 (90%)]\tLoss: 1.468792\n",
      "Epoch: 8 [0/2000 (0%)]\tLoss: 1.158911\n",
      "Epoch: 8 [200/2000 (10%)]\tLoss: 1.133645\n",
      "Epoch: 8 [400/2000 (20%)]\tLoss: 0.767835\n",
      "Epoch: 8 [600/2000 (30%)]\tLoss: 1.432865\n",
      "Epoch: 8 [800/2000 (40%)]\tLoss: 1.336655\n",
      "Epoch: 8 [1000/2000 (50%)]\tLoss: 1.123270\n",
      "Epoch: 8 [1200/2000 (60%)]\tLoss: 1.252154\n",
      "Epoch: 8 [1400/2000 (70%)]\tLoss: 1.164316\n",
      "Epoch: 8 [1600/2000 (80%)]\tLoss: 1.100350\n",
      "Epoch: 8 [1800/2000 (90%)]\tLoss: 0.943374\n",
      "Epoch: 9 [0/2000 (0%)]\tLoss: 1.189912\n",
      "Epoch: 9 [200/2000 (10%)]\tLoss: 1.560755\n",
      "Epoch: 9 [400/2000 (20%)]\tLoss: 0.965122\n",
      "Epoch: 9 [600/2000 (30%)]\tLoss: 1.243659\n",
      "Epoch: 9 [800/2000 (40%)]\tLoss: 1.470733\n",
      "Epoch: 9 [1000/2000 (50%)]\tLoss: 1.230267\n",
      "Epoch: 9 [1200/2000 (60%)]\tLoss: 1.266313\n",
      "Epoch: 9 [1400/2000 (70%)]\tLoss: 1.478885\n",
      "Epoch: 9 [1600/2000 (80%)]\tLoss: 0.825125\n",
      "Epoch: 9 [1800/2000 (90%)]\tLoss: 1.115183\n",
      "Epoch: 10 [0/2000 (0%)]\tLoss: 1.068423\n",
      "Epoch: 10 [200/2000 (10%)]\tLoss: 1.123271\n",
      "Epoch: 10 [400/2000 (20%)]\tLoss: 0.691072\n",
      "Epoch: 10 [600/2000 (30%)]\tLoss: 1.464020\n",
      "Epoch: 10 [800/2000 (40%)]\tLoss: 1.001022\n",
      "Epoch: 10 [1000/2000 (50%)]\tLoss: 0.815924\n",
      "Epoch: 10 [1200/2000 (60%)]\tLoss: 1.108404\n",
      "Epoch: 10 [1400/2000 (70%)]\tLoss: 1.254769\n",
      "Epoch: 10 [1600/2000 (80%)]\tLoss: 0.928727\n",
      "Epoch: 10 [1800/2000 (90%)]\tLoss: 1.120833\n",
      "Epoch: 11 [0/2000 (0%)]\tLoss: 0.878355\n",
      "Epoch: 11 [200/2000 (10%)]\tLoss: 0.903819\n",
      "Epoch: 11 [400/2000 (20%)]\tLoss: 0.813300\n",
      "Epoch: 11 [600/2000 (30%)]\tLoss: 0.653831\n",
      "Epoch: 11 [800/2000 (40%)]\tLoss: 0.577139\n",
      "Epoch: 11 [1000/2000 (50%)]\tLoss: 0.421461\n",
      "Epoch: 11 [1200/2000 (60%)]\tLoss: 1.698630\n",
      "Epoch: 11 [1400/2000 (70%)]\tLoss: 1.019073\n",
      "Epoch: 11 [1600/2000 (80%)]\tLoss: 0.374612\n",
      "Epoch: 11 [1800/2000 (90%)]\tLoss: 0.511798\n",
      "Epoch: 12 [0/2000 (0%)]\tLoss: 0.733077\n",
      "Epoch: 12 [200/2000 (10%)]\tLoss: 0.469889\n",
      "Epoch: 12 [400/2000 (20%)]\tLoss: 0.242950\n",
      "Epoch: 12 [600/2000 (30%)]\tLoss: 0.846090\n",
      "Epoch: 12 [800/2000 (40%)]\tLoss: 0.110476\n",
      "Epoch: 12 [1000/2000 (50%)]\tLoss: 0.272803\n",
      "Epoch: 12 [1200/2000 (60%)]\tLoss: 0.708238\n",
      "Epoch: 12 [1400/2000 (70%)]\tLoss: 0.238553\n",
      "Epoch: 12 [1600/2000 (80%)]\tLoss: 0.540590\n",
      "Epoch: 12 [1800/2000 (90%)]\tLoss: 0.727845\n",
      "Epoch: 13 [0/2000 (0%)]\tLoss: 0.805680\n",
      "Epoch: 13 [200/2000 (10%)]\tLoss: 1.138785\n",
      "Epoch: 13 [400/2000 (20%)]\tLoss: 0.522206\n",
      "Epoch: 13 [600/2000 (30%)]\tLoss: 0.580414\n",
      "Epoch: 13 [800/2000 (40%)]\tLoss: 0.138756\n",
      "Epoch: 13 [1000/2000 (50%)]\tLoss: 0.961678\n",
      "Epoch: 13 [1200/2000 (60%)]\tLoss: 0.580948\n",
      "Epoch: 13 [1400/2000 (70%)]\tLoss: 0.352670\n",
      "Epoch: 13 [1600/2000 (80%)]\tLoss: 0.521304\n",
      "Epoch: 13 [1800/2000 (90%)]\tLoss: 0.657283\n",
      "Epoch: 14 [0/2000 (0%)]\tLoss: 0.894383\n",
      "Epoch: 14 [200/2000 (10%)]\tLoss: 0.030300\n",
      "Epoch: 14 [400/2000 (20%)]\tLoss: 0.272403\n",
      "Epoch: 14 [600/2000 (30%)]\tLoss: 0.592842\n",
      "Epoch: 14 [800/2000 (40%)]\tLoss: 0.347504\n",
      "Epoch: 14 [1000/2000 (50%)]\tLoss: 0.703827\n",
      "Epoch: 14 [1200/2000 (60%)]\tLoss: 0.661025\n",
      "Epoch: 14 [1400/2000 (70%)]\tLoss: 0.274510\n",
      "Epoch: 14 [1600/2000 (80%)]\tLoss: 0.517009\n",
      "Epoch: 14 [1800/2000 (90%)]\tLoss: 0.319759\n",
      "Epoch: 15 [0/2000 (0%)]\tLoss: 0.874307\n",
      "Epoch: 15 [200/2000 (10%)]\tLoss: 0.021455\n",
      "Epoch: 15 [400/2000 (20%)]\tLoss: 0.412243\n",
      "Epoch: 15 [600/2000 (30%)]\tLoss: 1.217924\n",
      "Epoch: 15 [800/2000 (40%)]\tLoss: 0.953797\n",
      "Epoch: 15 [1000/2000 (50%)]\tLoss: 0.619955\n",
      "Epoch: 15 [1200/2000 (60%)]\tLoss: 0.465787\n",
      "Epoch: 15 [1400/2000 (70%)]\tLoss: 0.034157\n",
      "Epoch: 15 [1600/2000 (80%)]\tLoss: 0.029780\n",
      "Epoch: 15 [1800/2000 (90%)]\tLoss: 0.378542\n",
      "Epoch: 16 [0/2000 (0%)]\tLoss: 1.644069\n",
      "Epoch: 16 [200/2000 (10%)]\tLoss: 0.728347\n",
      "Epoch: 16 [400/2000 (20%)]\tLoss: 0.028710\n",
      "Epoch: 16 [600/2000 (30%)]\tLoss: 0.170047\n",
      "Epoch: 16 [800/2000 (40%)]\tLoss: 0.738715\n",
      "Epoch: 16 [1000/2000 (50%)]\tLoss: 0.011739\n",
      "Epoch: 16 [1200/2000 (60%)]\tLoss: 0.688946\n",
      "Epoch: 16 [1400/2000 (70%)]\tLoss: 0.176061\n",
      "Epoch: 16 [1600/2000 (80%)]\tLoss: 0.844079\n",
      "Epoch: 16 [1800/2000 (90%)]\tLoss: 0.157755\n",
      "Epoch: 17 [0/2000 (0%)]\tLoss: 0.798150\n",
      "Epoch: 17 [200/2000 (10%)]\tLoss: 0.005890\n",
      "Epoch: 17 [400/2000 (20%)]\tLoss: 0.244018\n",
      "Epoch: 17 [600/2000 (30%)]\tLoss: 0.233041\n",
      "Epoch: 17 [800/2000 (40%)]\tLoss: 0.084014\n",
      "Epoch: 17 [1000/2000 (50%)]\tLoss: 0.028000\n",
      "Epoch: 17 [1200/2000 (60%)]\tLoss: 0.057190\n",
      "Epoch: 17 [1400/2000 (70%)]\tLoss: 0.026409\n",
      "Epoch: 17 [1600/2000 (80%)]\tLoss: 0.467062\n",
      "Epoch: 17 [1800/2000 (90%)]\tLoss: 0.853131\n",
      "Epoch: 18 [0/2000 (0%)]\tLoss: 0.216083\n",
      "Epoch: 18 [200/2000 (10%)]\tLoss: 1.110214\n",
      "Epoch: 18 [400/2000 (20%)]\tLoss: 0.346167\n",
      "Epoch: 18 [600/2000 (30%)]\tLoss: 0.007377\n",
      "Epoch: 18 [800/2000 (40%)]\tLoss: 0.343053\n",
      "Epoch: 18 [1000/2000 (50%)]\tLoss: 0.047662\n",
      "Epoch: 18 [1200/2000 (60%)]\tLoss: 0.079071\n",
      "Epoch: 18 [1400/2000 (70%)]\tLoss: 0.452262\n",
      "Epoch: 18 [1600/2000 (80%)]\tLoss: 0.673082\n",
      "Epoch: 18 [1800/2000 (90%)]\tLoss: 0.412752\n",
      "Epoch: 19 [0/2000 (0%)]\tLoss: 0.409027\n",
      "Epoch: 19 [200/2000 (10%)]\tLoss: 0.087359\n",
      "Epoch: 19 [400/2000 (20%)]\tLoss: 0.152560\n",
      "Epoch: 19 [600/2000 (30%)]\tLoss: 0.183058\n",
      "Epoch: 19 [800/2000 (40%)]\tLoss: 0.038960\n",
      "Epoch: 19 [1000/2000 (50%)]\tLoss: 0.055037\n",
      "Epoch: 19 [1200/2000 (60%)]\tLoss: 0.482303\n",
      "Epoch: 19 [1400/2000 (70%)]\tLoss: 0.003291\n",
      "Epoch: 19 [1600/2000 (80%)]\tLoss: 0.286960\n",
      "Epoch: 19 [1800/2000 (90%)]\tLoss: 0.163873\n",
      "Epoch: 20 [0/2000 (0%)]\tLoss: 0.004140\n",
      "Epoch: 20 [200/2000 (10%)]\tLoss: 0.314175\n",
      "Epoch: 20 [400/2000 (20%)]\tLoss: 0.065236\n",
      "Epoch: 20 [600/2000 (30%)]\tLoss: 0.233650\n",
      "Epoch: 20 [800/2000 (40%)]\tLoss: 0.022824\n",
      "Epoch: 20 [1000/2000 (50%)]\tLoss: 0.092696\n",
      "Epoch: 20 [1200/2000 (60%)]\tLoss: 0.006775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 [1400/2000 (70%)]\tLoss: 1.858165\n",
      "Epoch: 20 [1600/2000 (80%)]\tLoss: 0.004092\n",
      "Epoch: 20 [1800/2000 (90%)]\tLoss: 0.002889\n",
      "Epoch: 21 [0/2000 (0%)]\tLoss: 0.000326\n",
      "Epoch: 21 [200/2000 (10%)]\tLoss: 0.351592\n",
      "Epoch: 21 [400/2000 (20%)]\tLoss: 0.612712\n",
      "Epoch: 21 [600/2000 (30%)]\tLoss: 0.000929\n",
      "Epoch: 21 [800/2000 (40%)]\tLoss: 0.002278\n",
      "Epoch: 21 [1000/2000 (50%)]\tLoss: 0.430526\n",
      "Epoch: 21 [1200/2000 (60%)]\tLoss: 0.004029\n",
      "Epoch: 21 [1400/2000 (70%)]\tLoss: 0.010619\n",
      "Epoch: 21 [1600/2000 (80%)]\tLoss: 0.100930\n",
      "Epoch: 21 [1800/2000 (90%)]\tLoss: 0.194808\n",
      "Epoch: 22 [0/2000 (0%)]\tLoss: 0.000309\n",
      "Epoch: 22 [200/2000 (10%)]\tLoss: 0.000480\n",
      "Epoch: 22 [400/2000 (20%)]\tLoss: 1.565876\n",
      "Epoch: 22 [600/2000 (30%)]\tLoss: 0.054466\n",
      "Epoch: 22 [800/2000 (40%)]\tLoss: 0.003180\n",
      "Epoch: 22 [1000/2000 (50%)]\tLoss: 0.523901\n",
      "Epoch: 22 [1200/2000 (60%)]\tLoss: 0.003319\n",
      "Epoch: 22 [1400/2000 (70%)]\tLoss: 0.169608\n",
      "Epoch: 22 [1600/2000 (80%)]\tLoss: 0.524826\n",
      "Epoch: 22 [1800/2000 (90%)]\tLoss: 0.027723\n",
      "Epoch: 23 [0/2000 (0%)]\tLoss: 0.001497\n",
      "Epoch: 23 [200/2000 (10%)]\tLoss: 0.000789\n",
      "Epoch: 23 [400/2000 (20%)]\tLoss: 0.035034\n",
      "Epoch: 23 [600/2000 (30%)]\tLoss: 0.007680\n",
      "Epoch: 23 [800/2000 (40%)]\tLoss: 0.826700\n",
      "Epoch: 23 [1000/2000 (50%)]\tLoss: 0.002091\n",
      "Epoch: 23 [1200/2000 (60%)]\tLoss: 0.045516\n",
      "Epoch: 23 [1400/2000 (70%)]\tLoss: 0.000147\n",
      "Epoch: 23 [1600/2000 (80%)]\tLoss: 0.255958\n",
      "Epoch: 23 [1800/2000 (90%)]\tLoss: 0.022736\n",
      "Epoch: 24 [0/2000 (0%)]\tLoss: 0.004072\n",
      "Epoch: 24 [200/2000 (10%)]\tLoss: 0.037140\n",
      "Epoch: 24 [400/2000 (20%)]\tLoss: 0.018735\n",
      "Epoch: 24 [600/2000 (30%)]\tLoss: 0.000442\n",
      "Epoch: 24 [800/2000 (40%)]\tLoss: 0.001965\n",
      "Epoch: 24 [1000/2000 (50%)]\tLoss: 0.082722\n",
      "Epoch: 24 [1200/2000 (60%)]\tLoss: 0.098457\n",
      "Epoch: 24 [1400/2000 (70%)]\tLoss: 0.066705\n",
      "Epoch: 24 [1600/2000 (80%)]\tLoss: 0.003606\n",
      "Epoch: 24 [1800/2000 (90%)]\tLoss: 0.008431\n",
      "Epoch: 25 [0/2000 (0%)]\tLoss: 0.000843\n",
      "Epoch: 25 [200/2000 (10%)]\tLoss: 0.001160\n",
      "Epoch: 25 [400/2000 (20%)]\tLoss: 0.032304\n",
      "Epoch: 25 [600/2000 (30%)]\tLoss: 0.220953\n",
      "Epoch: 25 [800/2000 (40%)]\tLoss: 0.036630\n",
      "Epoch: 25 [1000/2000 (50%)]\tLoss: 0.017462\n",
      "Epoch: 25 [1200/2000 (60%)]\tLoss: 0.051354\n",
      "Epoch: 25 [1400/2000 (70%)]\tLoss: 0.263422\n",
      "Epoch: 25 [1600/2000 (80%)]\tLoss: 0.001574\n",
      "Epoch: 25 [1800/2000 (90%)]\tLoss: 0.000202\n",
      "Epoch: 26 [0/2000 (0%)]\tLoss: 0.110236\n",
      "Epoch: 26 [200/2000 (10%)]\tLoss: 0.025135\n",
      "Epoch: 26 [400/2000 (20%)]\tLoss: 0.009349\n",
      "Epoch: 26 [600/2000 (30%)]\tLoss: 0.005554\n",
      "Epoch: 26 [800/2000 (40%)]\tLoss: 1.050170\n",
      "Epoch: 26 [1000/2000 (50%)]\tLoss: 0.217464\n",
      "Epoch: 26 [1200/2000 (60%)]\tLoss: 0.021422\n",
      "Epoch: 26 [1400/2000 (70%)]\tLoss: 2.535151\n",
      "Epoch: 26 [1600/2000 (80%)]\tLoss: 0.356900\n",
      "Epoch: 26 [1800/2000 (90%)]\tLoss: 0.003263\n",
      "Epoch: 27 [0/2000 (0%)]\tLoss: 0.000058\n",
      "Epoch: 27 [200/2000 (10%)]\tLoss: 0.850150\n",
      "Epoch: 27 [400/2000 (20%)]\tLoss: 0.027304\n",
      "Epoch: 27 [600/2000 (30%)]\tLoss: 0.002549\n",
      "Epoch: 27 [800/2000 (40%)]\tLoss: 0.031931\n",
      "Epoch: 27 [1000/2000 (50%)]\tLoss: 1.080798\n",
      "Epoch: 27 [1200/2000 (60%)]\tLoss: 0.002195\n",
      "Epoch: 27 [1400/2000 (70%)]\tLoss: 0.002950\n",
      "Epoch: 27 [1600/2000 (80%)]\tLoss: 0.189094\n",
      "Epoch: 27 [1800/2000 (90%)]\tLoss: 0.345832\n",
      "Epoch: 28 [0/2000 (0%)]\tLoss: 0.001754\n",
      "Epoch: 28 [200/2000 (10%)]\tLoss: 0.000157\n",
      "Epoch: 28 [400/2000 (20%)]\tLoss: 0.001923\n",
      "Epoch: 28 [600/2000 (30%)]\tLoss: 0.298344\n",
      "Epoch: 28 [800/2000 (40%)]\tLoss: 0.022157\n",
      "Epoch: 28 [1000/2000 (50%)]\tLoss: 0.002680\n",
      "Epoch: 28 [1200/2000 (60%)]\tLoss: 0.034391\n",
      "Epoch: 28 [1400/2000 (70%)]\tLoss: 0.001574\n",
      "Epoch: 28 [1600/2000 (80%)]\tLoss: 0.025523\n",
      "Epoch: 28 [1800/2000 (90%)]\tLoss: 0.004245\n",
      "Epoch: 29 [0/2000 (0%)]\tLoss: 0.001578\n",
      "Epoch: 29 [200/2000 (10%)]\tLoss: 0.000810\n",
      "Epoch: 29 [400/2000 (20%)]\tLoss: 0.000181\n",
      "Epoch: 29 [600/2000 (30%)]\tLoss: 0.000323\n",
      "Epoch: 29 [800/2000 (40%)]\tLoss: 0.045737\n",
      "Epoch: 29 [1000/2000 (50%)]\tLoss: 0.002560\n",
      "Epoch: 29 [1200/2000 (60%)]\tLoss: 0.000146\n",
      "Epoch: 29 [1400/2000 (70%)]\tLoss: 0.001936\n",
      "Epoch: 29 [1600/2000 (80%)]\tLoss: 0.000580\n",
      "Epoch: 29 [1800/2000 (90%)]\tLoss: 0.000071\n",
      "Epoch: 30 [0/2000 (0%)]\tLoss: 0.074103\n",
      "Epoch: 30 [200/2000 (10%)]\tLoss: 0.000169\n",
      "Epoch: 30 [400/2000 (20%)]\tLoss: 0.001085\n",
      "Epoch: 30 [600/2000 (30%)]\tLoss: 0.000424\n",
      "Epoch: 30 [800/2000 (40%)]\tLoss: 0.000418\n",
      "Epoch: 30 [1000/2000 (50%)]\tLoss: 0.032378\n",
      "Epoch: 30 [1200/2000 (60%)]\tLoss: 0.001606\n",
      "Epoch: 30 [1400/2000 (70%)]\tLoss: 0.000399\n",
      "Epoch: 30 [1600/2000 (80%)]\tLoss: 0.067237\n",
      "Epoch: 30 [1800/2000 (90%)]\tLoss: 0.000749\n",
      "Epoch: 31 [0/2000 (0%)]\tLoss: 0.001688\n",
      "Epoch: 31 [200/2000 (10%)]\tLoss: 0.044798\n",
      "Epoch: 31 [400/2000 (20%)]\tLoss: 0.007703\n",
      "Epoch: 31 [600/2000 (30%)]\tLoss: 0.010932\n",
      "Epoch: 31 [800/2000 (40%)]\tLoss: 0.001970\n",
      "Epoch: 31 [1000/2000 (50%)]\tLoss: 0.002028\n",
      "Epoch: 31 [1200/2000 (60%)]\tLoss: 0.004886\n",
      "Epoch: 31 [1400/2000 (70%)]\tLoss: 0.176216\n",
      "Epoch: 31 [1600/2000 (80%)]\tLoss: 0.000007\n",
      "Epoch: 31 [1800/2000 (90%)]\tLoss: 0.009672\n",
      "Epoch: 32 [0/2000 (0%)]\tLoss: 0.088319\n",
      "Epoch: 32 [200/2000 (10%)]\tLoss: 0.003214\n",
      "Epoch: 32 [400/2000 (20%)]\tLoss: 0.000359\n",
      "Epoch: 32 [600/2000 (30%)]\tLoss: 0.004890\n",
      "Epoch: 32 [800/2000 (40%)]\tLoss: 0.000662\n",
      "Epoch: 32 [1000/2000 (50%)]\tLoss: 0.038638\n",
      "Epoch: 32 [1200/2000 (60%)]\tLoss: 0.021805\n",
      "Epoch: 32 [1400/2000 (70%)]\tLoss: 0.003184\n",
      "Epoch: 32 [1600/2000 (80%)]\tLoss: 0.001902\n",
      "Epoch: 32 [1800/2000 (90%)]\tLoss: 0.039682\n",
      "Epoch: 33 [0/2000 (0%)]\tLoss: 0.000054\n",
      "Epoch: 33 [200/2000 (10%)]\tLoss: 0.000380\n",
      "Epoch: 33 [400/2000 (20%)]\tLoss: 0.000681\n",
      "Epoch: 33 [600/2000 (30%)]\tLoss: 0.000278\n",
      "Epoch: 33 [800/2000 (40%)]\tLoss: 0.000184\n",
      "Epoch: 33 [1000/2000 (50%)]\tLoss: 0.001019\n",
      "Epoch: 33 [1200/2000 (60%)]\tLoss: 0.058113\n",
      "Epoch: 33 [1400/2000 (70%)]\tLoss: 0.002143\n",
      "Epoch: 33 [1600/2000 (80%)]\tLoss: 0.000032\n",
      "Epoch: 33 [1800/2000 (90%)]\tLoss: 0.000875\n",
      "Epoch: 34 [0/2000 (0%)]\tLoss: 0.000457\n",
      "Epoch: 34 [200/2000 (10%)]\tLoss: 0.004301\n",
      "Epoch: 34 [400/2000 (20%)]\tLoss: 0.000233\n",
      "Epoch: 34 [600/2000 (30%)]\tLoss: 0.018163\n",
      "Epoch: 34 [800/2000 (40%)]\tLoss: 0.002149\n",
      "Epoch: 34 [1000/2000 (50%)]\tLoss: 0.026818\n",
      "Epoch: 34 [1200/2000 (60%)]\tLoss: 0.001149\n",
      "Epoch: 34 [1400/2000 (70%)]\tLoss: 0.001387\n",
      "Epoch: 34 [1600/2000 (80%)]\tLoss: 0.011455\n",
      "Epoch: 34 [1800/2000 (90%)]\tLoss: 0.002333\n",
      "Epoch: 35 [0/2000 (0%)]\tLoss: 0.000290\n",
      "Epoch: 35 [200/2000 (10%)]\tLoss: 0.002737\n",
      "Epoch: 35 [400/2000 (20%)]\tLoss: 0.000969\n",
      "Epoch: 35 [600/2000 (30%)]\tLoss: 0.002756\n",
      "Epoch: 35 [800/2000 (40%)]\tLoss: 0.000629\n",
      "Epoch: 35 [1000/2000 (50%)]\tLoss: 0.003680\n",
      "Epoch: 35 [1200/2000 (60%)]\tLoss: 0.005633\n",
      "Epoch: 35 [1400/2000 (70%)]\tLoss: 0.000467\n",
      "Epoch: 35 [1600/2000 (80%)]\tLoss: 0.000917\n",
      "Epoch: 35 [1800/2000 (90%)]\tLoss: 0.002157\n",
      "Epoch: 36 [0/2000 (0%)]\tLoss: 0.001237\n",
      "Epoch: 36 [200/2000 (10%)]\tLoss: 0.000457\n",
      "Epoch: 36 [400/2000 (20%)]\tLoss: 0.002128\n",
      "Epoch: 36 [600/2000 (30%)]\tLoss: 0.246757\n",
      "Epoch: 36 [800/2000 (40%)]\tLoss: 0.005287\n",
      "Epoch: 36 [1000/2000 (50%)]\tLoss: 0.002709\n",
      "Epoch: 36 [1200/2000 (60%)]\tLoss: 0.000096\n",
      "Epoch: 36 [1400/2000 (70%)]\tLoss: 0.002197\n",
      "Epoch: 36 [1600/2000 (80%)]\tLoss: 0.014117\n",
      "Epoch: 36 [1800/2000 (90%)]\tLoss: 0.000140\n",
      "Epoch: 37 [0/2000 (0%)]\tLoss: 0.000070\n",
      "Epoch: 37 [200/2000 (10%)]\tLoss: 0.001252\n",
      "Epoch: 37 [400/2000 (20%)]\tLoss: 0.001455\n",
      "Epoch: 37 [600/2000 (30%)]\tLoss: 0.000674\n",
      "Epoch: 37 [800/2000 (40%)]\tLoss: 0.000233\n",
      "Epoch: 37 [1000/2000 (50%)]\tLoss: 0.000061\n",
      "Epoch: 37 [1200/2000 (60%)]\tLoss: 0.000285\n",
      "Epoch: 37 [1400/2000 (70%)]\tLoss: 0.019343\n",
      "Epoch: 37 [1600/2000 (80%)]\tLoss: 0.000085\n",
      "Epoch: 37 [1800/2000 (90%)]\tLoss: 0.000010\n",
      "Epoch: 38 [0/2000 (0%)]\tLoss: 0.000577\n",
      "Epoch: 38 [200/2000 (10%)]\tLoss: 0.000661\n",
      "Epoch: 38 [400/2000 (20%)]\tLoss: 0.001112\n",
      "Epoch: 38 [600/2000 (30%)]\tLoss: 0.108527\n",
      "Epoch: 38 [800/2000 (40%)]\tLoss: 0.008104\n",
      "Epoch: 38 [1000/2000 (50%)]\tLoss: 0.002315\n",
      "Epoch: 38 [1200/2000 (60%)]\tLoss: 0.003225\n",
      "Epoch: 38 [1400/2000 (70%)]\tLoss: 0.000188\n",
      "Epoch: 38 [1600/2000 (80%)]\tLoss: 0.002731\n",
      "Epoch: 38 [1800/2000 (90%)]\tLoss: 0.005578\n",
      "Epoch: 39 [0/2000 (0%)]\tLoss: 0.005745\n",
      "Epoch: 39 [200/2000 (10%)]\tLoss: 0.001069\n",
      "Epoch: 39 [400/2000 (20%)]\tLoss: 0.000801\n",
      "Epoch: 39 [600/2000 (30%)]\tLoss: 0.000716\n",
      "Epoch: 39 [800/2000 (40%)]\tLoss: 0.000003\n",
      "Epoch: 39 [1000/2000 (50%)]\tLoss: 0.000020\n",
      "Epoch: 39 [1200/2000 (60%)]\tLoss: 0.000434\n",
      "Epoch: 39 [1400/2000 (70%)]\tLoss: 0.000161\n",
      "Epoch: 39 [1600/2000 (80%)]\tLoss: 0.000313\n",
      "Epoch: 39 [1800/2000 (90%)]\tLoss: 0.000611\n",
      "Epoch: 40 [0/2000 (0%)]\tLoss: 0.000001\n",
      "Epoch: 40 [200/2000 (10%)]\tLoss: 0.001293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 [400/2000 (20%)]\tLoss: 0.000223\n",
      "Epoch: 40 [600/2000 (30%)]\tLoss: 0.000166\n",
      "Epoch: 40 [800/2000 (40%)]\tLoss: 0.000204\n",
      "Epoch: 40 [1000/2000 (50%)]\tLoss: 0.000178\n",
      "Epoch: 40 [1200/2000 (60%)]\tLoss: 0.000121\n",
      "Epoch: 40 [1400/2000 (70%)]\tLoss: 0.000093\n",
      "Epoch: 40 [1600/2000 (80%)]\tLoss: 0.000629\n",
      "Epoch: 40 [1800/2000 (90%)]\tLoss: 0.000108\n",
      "Epoch: 41 [0/2000 (0%)]\tLoss: 0.000061\n",
      "Epoch: 41 [200/2000 (10%)]\tLoss: 0.005745\n",
      "Epoch: 41 [400/2000 (20%)]\tLoss: 0.000684\n",
      "Epoch: 41 [600/2000 (30%)]\tLoss: 0.000243\n",
      "Epoch: 41 [800/2000 (40%)]\tLoss: 0.000094\n",
      "Epoch: 41 [1000/2000 (50%)]\tLoss: 0.001021\n",
      "Epoch: 41 [1200/2000 (60%)]\tLoss: 0.000026\n",
      "Epoch: 41 [1400/2000 (70%)]\tLoss: 0.000490\n",
      "Epoch: 41 [1600/2000 (80%)]\tLoss: 0.000190\n",
      "Epoch: 41 [1800/2000 (90%)]\tLoss: 0.000250\n",
      "Epoch: 42 [0/2000 (0%)]\tLoss: 0.000287\n",
      "Epoch: 42 [200/2000 (10%)]\tLoss: 0.000052\n",
      "Epoch: 42 [400/2000 (20%)]\tLoss: 0.000413\n",
      "Epoch: 42 [600/2000 (30%)]\tLoss: 0.000304\n",
      "Epoch: 42 [800/2000 (40%)]\tLoss: 0.000426\n",
      "Epoch: 42 [1000/2000 (50%)]\tLoss: 0.000115\n",
      "Epoch: 42 [1200/2000 (60%)]\tLoss: 0.000011\n",
      "Epoch: 42 [1400/2000 (70%)]\tLoss: 0.000691\n",
      "Epoch: 42 [1600/2000 (80%)]\tLoss: 0.000588\n",
      "Epoch: 42 [1800/2000 (90%)]\tLoss: 0.000026\n",
      "Epoch: 43 [0/2000 (0%)]\tLoss: 0.000148\n",
      "Epoch: 43 [200/2000 (10%)]\tLoss: 0.000062\n",
      "Epoch: 43 [400/2000 (20%)]\tLoss: 0.000051\n",
      "Epoch: 43 [600/2000 (30%)]\tLoss: 0.000071\n",
      "Epoch: 43 [800/2000 (40%)]\tLoss: 0.000019\n",
      "Epoch: 43 [1000/2000 (50%)]\tLoss: 0.000036\n",
      "Epoch: 43 [1200/2000 (60%)]\tLoss: 0.000003\n",
      "Epoch: 43 [1400/2000 (70%)]\tLoss: 0.001503\n",
      "Epoch: 43 [1600/2000 (80%)]\tLoss: 0.000069\n",
      "Epoch: 43 [1800/2000 (90%)]\tLoss: 0.000148\n",
      "Epoch: 44 [0/2000 (0%)]\tLoss: 0.000010\n",
      "Epoch: 44 [200/2000 (10%)]\tLoss: 0.000018\n",
      "Epoch: 44 [400/2000 (20%)]\tLoss: 0.000195\n",
      "Epoch: 44 [600/2000 (30%)]\tLoss: 0.000588\n",
      "Epoch: 44 [800/2000 (40%)]\tLoss: 0.000212\n",
      "Epoch: 44 [1000/2000 (50%)]\tLoss: 0.000261\n",
      "Epoch: 44 [1200/2000 (60%)]\tLoss: 0.000020\n",
      "Epoch: 44 [1400/2000 (70%)]\tLoss: 0.000815\n",
      "Epoch: 44 [1600/2000 (80%)]\tLoss: 0.000022\n",
      "Epoch: 44 [1800/2000 (90%)]\tLoss: 0.000400\n",
      "Epoch: 45 [0/2000 (0%)]\tLoss: 0.000089\n",
      "Epoch: 45 [200/2000 (10%)]\tLoss: 0.001849\n",
      "Epoch: 45 [400/2000 (20%)]\tLoss: 0.000079\n",
      "Epoch: 45 [600/2000 (30%)]\tLoss: 0.000224\n",
      "Epoch: 45 [800/2000 (40%)]\tLoss: 0.000017\n",
      "Epoch: 45 [1000/2000 (50%)]\tLoss: 0.000220\n",
      "Epoch: 45 [1200/2000 (60%)]\tLoss: 0.002366\n",
      "Epoch: 45 [1400/2000 (70%)]\tLoss: 0.000052\n",
      "Epoch: 45 [1600/2000 (80%)]\tLoss: 0.000091\n",
      "Epoch: 45 [1800/2000 (90%)]\tLoss: 0.000306\n",
      "Epoch: 46 [0/2000 (0%)]\tLoss: 0.002995\n",
      "Epoch: 46 [200/2000 (10%)]\tLoss: 0.000008\n",
      "Epoch: 46 [400/2000 (20%)]\tLoss: 0.000130\n",
      "Epoch: 46 [600/2000 (30%)]\tLoss: 0.000649\n",
      "Epoch: 46 [800/2000 (40%)]\tLoss: 0.000587\n",
      "Epoch: 46 [1000/2000 (50%)]\tLoss: 0.000001\n",
      "Epoch: 46 [1200/2000 (60%)]\tLoss: 0.000237\n",
      "Epoch: 46 [1400/2000 (70%)]\tLoss: 0.000071\n",
      "Epoch: 46 [1600/2000 (80%)]\tLoss: 0.000056\n",
      "Epoch: 46 [1800/2000 (90%)]\tLoss: 0.000055\n",
      "Epoch: 47 [0/2000 (0%)]\tLoss: 0.000105\n",
      "Epoch: 47 [200/2000 (10%)]\tLoss: 0.000056\n",
      "Epoch: 47 [400/2000 (20%)]\tLoss: 0.000021\n",
      "Epoch: 47 [600/2000 (30%)]\tLoss: 0.000014\n",
      "Epoch: 47 [800/2000 (40%)]\tLoss: 0.000038\n",
      "Epoch: 47 [1000/2000 (50%)]\tLoss: 0.000002\n",
      "Epoch: 47 [1200/2000 (60%)]\tLoss: 0.000946\n",
      "Epoch: 47 [1400/2000 (70%)]\tLoss: 0.001413\n",
      "Epoch: 47 [1600/2000 (80%)]\tLoss: 0.000011\n",
      "Epoch: 47 [1800/2000 (90%)]\tLoss: 0.000017\n",
      "Epoch: 48 [0/2000 (0%)]\tLoss: 0.000447\n",
      "Epoch: 48 [200/2000 (10%)]\tLoss: 0.000083\n",
      "Epoch: 48 [400/2000 (20%)]\tLoss: 0.000095\n",
      "Epoch: 48 [600/2000 (30%)]\tLoss: 0.000003\n",
      "Epoch: 48 [800/2000 (40%)]\tLoss: 0.000198\n",
      "Epoch: 48 [1000/2000 (50%)]\tLoss: 0.000220\n",
      "Epoch: 48 [1200/2000 (60%)]\tLoss: 0.000141\n",
      "Epoch: 48 [1400/2000 (70%)]\tLoss: 0.000207\n",
      "Epoch: 48 [1600/2000 (80%)]\tLoss: 0.000337\n",
      "Epoch: 48 [1800/2000 (90%)]\tLoss: 0.000054\n",
      "Epoch: 49 [0/2000 (0%)]\tLoss: 0.000058\n",
      "Epoch: 49 [200/2000 (10%)]\tLoss: 0.000358\n",
      "Epoch: 49 [400/2000 (20%)]\tLoss: 0.000164\n",
      "Epoch: 49 [600/2000 (30%)]\tLoss: 0.000036\n",
      "Epoch: 49 [800/2000 (40%)]\tLoss: 0.001041\n",
      "Epoch: 49 [1000/2000 (50%)]\tLoss: 0.000041\n",
      "Epoch: 49 [1200/2000 (60%)]\tLoss: 0.000011\n",
      "Epoch: 49 [1400/2000 (70%)]\tLoss: 0.000025\n",
      "Epoch: 49 [1600/2000 (80%)]\tLoss: 0.000114\n",
      "Epoch: 49 [1800/2000 (90%)]\tLoss: 0.000018\n",
      "Epoch: 50 [0/2000 (0%)]\tLoss: 0.000138\n",
      "Epoch: 50 [200/2000 (10%)]\tLoss: 0.000376\n",
      "Epoch: 50 [400/2000 (20%)]\tLoss: 0.000070\n",
      "Epoch: 50 [600/2000 (30%)]\tLoss: 0.000079\n",
      "Epoch: 50 [800/2000 (40%)]\tLoss: 0.000073\n",
      "Epoch: 50 [1000/2000 (50%)]\tLoss: 0.000018\n",
      "Epoch: 50 [1200/2000 (60%)]\tLoss: 0.000013\n",
      "Epoch: 50 [1400/2000 (70%)]\tLoss: 0.000454\n",
      "Epoch: 50 [1600/2000 (80%)]\tLoss: 0.000132\n",
      "Epoch: 50 [1800/2000 (90%)]\tLoss: 0.000726\n",
      "Finished training in  489.862 seconds \n",
      "\n",
      "Test set: Average loss: 0.3005, Accuracy: 67/100 (67%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbd074df978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "#Import\n",
    "data_transform = transforms.Compose(\n",
    "    [transforms.Grayscale(),\n",
    "     transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5,0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "train_set = datasets.ImageFolder(root='clouds_medium',\n",
    "                                transform=data_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                             batch_size=4, shuffle=True,\n",
    "                                             num_workers=1)\n",
    "\n",
    "test_set = datasets.ImageFolder(root='clouds_medium_test',\n",
    "                                transform=data_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                             batch_size=4,shuffle=False,\n",
    "                                             num_workers=1)\n",
    "\n",
    "cloud_classes = ('0', 'pi/4', 'pi/2', '3pi/4')\n",
    "\n",
    "#Model\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(32 * 32, 200)\n",
    "            self.fc2 = nn.Linear(200, 200)\n",
    "            self.fc3 = nn.Linear(200, 4)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return F.log_softmax(input=x)\n",
    "model = Net()\n",
    "\n",
    "#Optimizer\n",
    "import torch.optim as optim\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "#Training\n",
    "import time\n",
    "start_time = time.time()\n",
    "print(\"Started training\")\n",
    "\n",
    "epochs = 50\n",
    "print_interval = 50\n",
    "tempo = []\n",
    "acc = []\n",
    "\n",
    "for epoch in range(epochs):  \n",
    "    for batch_idx, (data, target) in enumerate(train_loader): \n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        data = data.view(-1, 32*32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        net_out = model(data)\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % print_interval == 0:\n",
    "            print('Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "    tempo.append(epoch)\n",
    "    acc.append(loss.data[0])\n",
    "    \n",
    "print(\"Finished training in  %.3f seconds \" % (time.time() - start_time))\n",
    "\n",
    "#Testing\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    \n",
    "    data = data.view(-1, 32 * 32)\n",
    "    net_out = model(data)\n",
    "    \n",
    "    test_loss += criterion(net_out, target).data[0]\n",
    "    pred = net_out.data.max(1)[1]\n",
    "    correct += pred.eq(target.data).sum() \n",
    "\n",
    "test_loss /= len(test_loader.dataset) \n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "#Saving\n",
    "torch.save(model.state_dict(), \"MODEL_trainMEDIUM_pytorchMCV2\")\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(tempo, acc)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss variation - Medium training')\n",
    "plt.savefig('Loss_medtraining.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et pour terminer avec le dur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "#Import\n",
    "data_transform = transforms.Compose(\n",
    "    [transforms.Grayscale(),\n",
    "     transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5,0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "train_set = datasets.ImageFolder(root='clouds_hard',\n",
    "                                transform=data_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                             batch_size=4, shuffle=True,\n",
    "                                             num_workers=1)\n",
    "\n",
    "test_set = datasets.ImageFolder(root='clouds_hard_test',\n",
    "                                transform=data_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                             batch_size=4,shuffle=False,\n",
    "                                             num_workers=1)\n",
    "\n",
    "cloud_classes = ('0', 'pi/4', 'pi/2', '3pi/4')\n",
    "\n",
    "#Model\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(32 * 32, 200)\n",
    "            self.fc2 = nn.Linear(200, 200)\n",
    "            self.fc3 = nn.Linear(200, 4)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return F.log_softmax(input=x)\n",
    "model = Net()\n",
    "\n",
    "#Optimizer\n",
    "import torch.optim as optim\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "#Training\n",
    "import time\n",
    "start_time = time.time()\n",
    "print(\"Started training\")\n",
    "\n",
    "epochs = 50\n",
    "print_interval = 50\n",
    "tempo = []\n",
    "acc = []\n",
    "\n",
    "for epoch in range(epochs):  \n",
    "    for batch_idx, (data, target) in enumerate(train_loader): \n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        data = data.view(-1, 32*32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        net_out = model(data)\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % print_interval == 0:\n",
    "            print('Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "    tempo.append(epoch)\n",
    "    acc.append(loss.data[0])\n",
    "    \n",
    "print(\"Finished training in  %.3f seconds \" % (time.time() - start_time))\n",
    "\n",
    "#Testing\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    \n",
    "    data = data.view(-1, 32 * 32)\n",
    "    net_out = model(data)\n",
    "    \n",
    "    test_loss += criterion(net_out, target).data[0]\n",
    "    pred = net_out.data.max(1)[1]\n",
    "    correct += pred.eq(target.data).sum() \n",
    "\n",
    "test_loss /= len(test_loader.dataset) \n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "#Saving\n",
    "torch.save(model.state_dict(), \"MODEL_trainHARD_pytorchMCV2\")\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(tempo, acc)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss variation - Hard training')\n",
    "plt.savefig('Loss_hardtraining.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
